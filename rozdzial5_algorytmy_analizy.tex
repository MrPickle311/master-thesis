\section{Zaimplementowane algorytmy analizy danych}
\label{sec:algorytmy_analizy}
\addcontentsline{lof}{section}{Rozdział \ref{sec:algorytmy_analizy}}

W niniejszym rozdziale omówiono algorytmy wykorzystane do analizy danych, zaimplementowane w ramach opracowanego systemu. System ten, nazwany \textit{SparkDataProcessor}, został zrealizowany w języku Scala z wykorzystaniem frameworka \mbox{\textit{Apache Spark}}~\cite{spark_streaming}. Jego głównym zadaniem jest przetwarzanie strumieni danych sensorycznych pochodzących z symulowanych urządzeń przemysłowych, takich jak: pompy, sprężarki i turbiny, szczegółowo opisane w rozdziale~\ref{sec:implementacja_generowania}. Aplikacja analizuje cztery podstawowe wielkości mierzone: ciśnienie, temperaturę, wilgotność oraz wibracje.

Architektura aplikacji \textit{SparkDataProcessor} opiera się na komponencie \textit{SparkDataProcessor}, inicjalizującego i zarządzającego poszczególnymi zadaniami przetwarzania strumieniowego. Jej działanie jest w pełni konfigurowalne za pomocą pliku \textit{application.conf} w formacie \textit{HOCON} (format konfiguracyjny dla aplikacji)~\cite{hocon_spec}, ładowanego przy użyciu biblioteki \textit{PureConfig}~\cite{pureconfig_docs} poprzez obiekt \textit{AppConfig}. Konfiguracja ta pozwala na dynamiczne włączanie i wyłączanie poszczególnych modułów analitycznych oraz precyzyjne definiowanie ich parametrów, takich jak: adresy brokerów \mbox{Apache Kafka}~\cite{kafka}, nazwy tematów wejściowych i wyjściowych, rozmiary okien czasowych dla agregacji, progi alarmowe dla detekcji zdarzeń, a także flagi debugowania.

Podstawą przetwarzanych informacji są modele danych zdefiniowane jako klasy przypadku (case classes) w języku Scala, znajdujące się w pakiecie \textit{com.factory.model}. Reprezentują one odczyty z poszczególnych sensorów (np. \textit{Pressure}, \textit{Temperature}, \textit{Humidity}, \textit{Vibration}) oraz strukturę wykrytych zdarzeń (\textit{Event}).

Aplikacja \textit{SparkDataProcessor} składa się z trzech głównych modułów przetwarzających dane:

Pierwszym z nich jest moduł do obliczania średnich wartości wielkości mierzonych przez czujniki \textit{MeanProcessor}, odpowiedzialny za obliczanie średnich wartości dla poszczególnych typów sensorów w zdefiniowanych oknach czasowych. Przetwarzanie to odbywa się dla każdego typu urządzenia (np. \("\)pump\("\), \("\)compressor\("\), \("\)turbine\("\)), identyfikowanego za pomocą etykiet (labels) w danych wejściowych. Moduł ten konsumuje informacje w formacie \mbox{\textit{Apache Avro}}~\cite{avro_documentation} z dedykowanych tematów brokera \mbox{\textit{Apache Kafka}}~\cite{kafka}. Aplikacja \textit{MeanProcessor} wykorzystuje mechanizmy biblioteki \mbox{\textit{Spark Structured Streaming}}, w tym funkcje okienkowania (np. \textit{window(col("event\_time"), "2 minutes", "1 minutes")}) oraz znaki wodne~(\textit{watermarks}) \\ \cite{watermarking} do obsługi wolumenów opóźnionych. Zagregowane wartości średnie, wraz z liczbą odczytów, są następnie serializowane z powrotem do formatu Avro i publikowane na odpowiednie tematy Kafka (np. \textit{pressureMean}).

Następny moduł \textit{EventProcessor}, ma za zadanie monitorowanie strumieni danych sensorycznych pod kątem przekroczenia zdefiniowanych progów ostrzegawczych i krytycznych. Analiza ta jest prowadzona indywidualnie dla każdego typu sensora i etykiety urządzenia. Ten komponent wykorzystuje operację \textit{flatMapGroupsWithState} dostępną w \mbox{\textit{Spark Structured Streaming}}~\cite{spark_streaming} , która pozwala na utrzymanie stanu dla każdego klucza (czyli dla każdej etykiety sensora). Moduł ten implementuje mechanizm \("\)cooldown\("\), zapobiegający generowaniu nadmiernej liczby alertów dla tego samego problemu w krótkim czasie. Okres wyciszenia może być zdefiniowany globalnie lub specyficznie dla danego typu sensora. Wykryte zdarzenia, zawierające m.in. tytuł, znacznik czasowy oraz status alertu (ostrzeżenie/krytyczny), są serializowane do formatu Avro przy użyciu schematu pobranego z usługi Confluent Schema Registry~\cite{confluent_schema_registry} i wysyłane do wspólnego tematu brokera Kafki o nazwie \textit{events}.

Trzecim, najbardziej złożonym modułem, jest \textit{EquipmentStateProcessor}. Jego celem jest predykcja ogólnego stanu technicznego monitorowanego urządzenia na podstawie połączonych danych z wielu sensorów. Moduł ten subskrybuje dane w formacie \mbox{\textit{Apache Avro}}~\cite{avro_documentation} z czterech głównych tematów brokera \mbox{\textit{Apache Kafka}} odpowiadających strumieniom ciśnienia, temperatury, wilgotności i wibracji. Każdy ze strumieni jest opatrzony znakiem wodnym w celu poprawnej obsługi danych napływających z opóźnieniem. Następnie strumienie te są łączone (join) na podstawie wspólnego klucza, który, zgodnie z dostarczonymi informacjami, reprezentuje identyfikator cyklu pomiarowego (kombinacja znacznika czasowego i typu urządzenia). Pozwala to na skorelowanie odczytów z różnych sensorów dla tego samego momentu i urządzenia. Połączone dane, zawierające cechy takie jak: typ urządzenia (\textit{equipment\_type}), znacznik czasowy zdarzenia (\textit{event\_timestamp}) oraz wartości poszczególnych pomiarów, są przekazywane do wytrenowanego wcześniej modelu uczenia maszynowego. Model ten, będący klasyfikatorem \textit{RandomForestClassifier} z biblioteki \mbox{\textit{Spark ML}} (ang. \mbox{Machine Learning Library})~\cite{spark_mllib_reference}, został wcześniej wytrenowany oraz został ładowany do aplikacji \mbox{\textit{Apache Spark}}. Na podstawie tych danych model klasyfikuje stan urządzenia. Możliwe stany, szczegółowo opisane w rozdziale~\ref{sec:implementacja_generowania}, obejmują: \("\)Stan normalny\("\), \("\)Wczesne zużycie\("\), \("\)Stan podkrytyczny\("\), \("\)Stan krytyczny\("\) oraz \("\)Naprawa\("\). Oryginalne dane sensoryczne są następnie wzbogacane o tę przewidzianą etykietę stanu (\textit{predicted\_status}). Tak przygotowane, wzbogacone dane dla każdego typu sensora (np. dane o ciśnieniu wraz z przewidywanym stanem) są serializowane do formatu Avro, z wykorzystaniem odpowiednich schematów (np. \textit{pressureAugumented-value}) pobieranych z Confluent Schema Registry~\cite{confluent_schema_registry} i publikowane na nowe, dedykowane tematy Kafka (np. \textit{pressureAugumented}). Głównym zastosowaniem tych wzbogaconych danych jest ich wizualizacja na wykresach w interfejsie użytkownika.

\subsection{Proces uczenia modelu predykcji stanu urządzenia}
\label{subsec:uczenie_modelu_stanu}

Model klasyfikacji \textit{RandomForestClassifier}, wykorzystywany w niniejszym systemie przez moduł \textit{EquipmentStateProcessor} do predykcji stanu technicznego urządzeń, jest trenowany w osobnym procesie wsadowym przy użyciu dedykowanego skryptu Python z biblioteką \mbox{\textit{PySpark}} (interfejs języka Python dla \mbox{\textit{Apache Spark}})~\cite{pyspark_docs}. Proces ten można podzielić na kilka kluczowych etapów.

\vspace{0.3em}

\begin{enumerate}
    \setlength\itemsep{0.5em}

    \item \textbf{Przygotowanie danych}

Źródłem danych treningowych dla modelu zaimplementowanego w niniejszej pracy są pliki CSV (format plików z wartościami rozdzielonymi przecinkami)~\cite{csv_rfc} generowane przez symulator. Zawierają one historyczne odczyty sensorów (temperatura, ciśnienie, wibracje, wilgotność) oraz odpowiadający im rzeczywisty stan urządzenia (np. \("\)Stan normalny\("\), \("\)Wczesne zużycie\("\), itd.). Dane dla różnych typów urządzeń są wczytywane do zmiennej Spark DataFrame (rozproszona kolekcja danych zorganizowana w nazwane kolumny)~\cite{chambers2018spark} , a następnie łączone w jeden zbiór. Przeprowadzane jest czyszczenie danych, obejmujące m.in. usunięcie rekordów ze stanem \textit{repair}, ponieważ ten stan nie jest przedmiotem klasyfikacji w czasie rzeczywistym, lecz wynikiem działania serwisowego. Kluczowym krokiem jest konwersja kolumny znacznika czasu (np. \textit{timestamp}) do odpowiedniego formatu \mbox{\textit{Spark TimestampType}} i sortowanie danych chronologicznie. Cały zbiór danych jest następnie dzielony na zbiory: treningowy (70\% danych), walidacyjny (15\%) oraz testowy (15\%). Podział ten realizowany jest w oparciu o kolejność czasową rekordów, co uzyskuje się przez dodanie kolumny z monotonicznie rosnącym ID po sortowaniu (\textit{monotonically\_increasing\_id()}) i filtrowanie na podstawie progów wyliczonych z proporcji. Taki sposób podziału ma na celu lepsze symulowanie rzeczywistych warunków predykcji przyszłych stanów na podstawie danych historycznych.

 \item \textbf{Inżynieria cech}

W ramach implementacji, w tym etapie przygotowywane są cechy wejściowe dla modelu. Kolumny reprezentujące odczyty sensorów (\textit{temperature}, \textit{pressure}, \textit{vibration}, \textit{humidity}) są rzutowane na typ \textit{double}. Zmienna docelowa, czyli kolumna \textit{status} opisująca stan urządzenia, jest konwertowana na wartości numeryczne przy użyciu transformatora \textit{StringIndexer} (transformator \mbox{\textit{Spark ML}} konwertujący kolumnę etykiet tekstowych na kolumnę indeksów liczbowych)~\cite{spark_mllib_reference}, tworząc nową kolumnę \textit{label}. Etykiety tekstowe, mapowane przez \\ \textit{StringIndexer} na indeksy, są zapamiętywane na potrzeby późniejszej konwersji predykcji z powrotem na tekst. Następnie, wybrane cechy numeryczne są łączone w jeden wektor cech za pomocą transformatora \textit{VectorAssembler} (transformator \mbox{\textit{Spark ML}} łączący wiele kolumn w jedną kolumnę wektorową)~\cite{spark_mllib_reference} , tworzący kolumnę \textit{features}.

    \item \textbf{Definicja i trening modelu}

Jako model klasyfikacyjny w niniejszej pracy wykorzystywany jest klasyfikator \\ \textit{RandomForestClassifier} z biblioteki \mbox{\textit{Spark MLlib}}~\cite{spark_mllib_reference}, skonfigurowany m.in. z liczbą drzew decyżejczych ustawioną na 100 (\textit{numTrees=100}) oraz stałym ziarnem losowości (\textit{seed=42}) dla zapewnienia reprodukowalności wyników. Cały proces transformacji danych i treningu modelu jest zdefiniowany jako potok (pipeline) \mbox{\textit{Spark ML}}, składający się z sekwencji kroków: \textit{StringIndexer} (dla kolumny \textit{status}), \textit{VectorAssembler} (dla cech wejściowych) oraz \textit{RandomForestClassifier}. Model jest trenowany poprzez wywołanie metody \textit{fit()} na tym potoku, używając przygotowanego wcześniej zbioru treningowego.

    \item \textbf{Zapis i przygotowanie modelu do inferencji}

Po zakończeniu treningu, wytrenowany potok jest zapisywany na dysku. Skrypt treningowy generuje dwie główne struktury modelu. Pierwsza, określana jako \("\)pełny potok deweloperski\("\) (zapisywana w ścieżce \textit{spark\_model\_path + "\_dev\_full\_pipeline"}), zawiera wszystkie etapy, łącznie ze \textit{StringIndexer}, i jest używana na potrzeby ewaluacji i dalszych prac deweloperskich w środowisku Python. Druga wersja, kluczowa dla aplikacji \textit{SparkDataProcessor}, jest zapisywana bezpośrednio pod ścieżką docelową dla modelu inferencyjnego. Ten model inferencyjny jest specjalnie przygotowanym potokiem, składającego się z funkcji transformującej i agregującej wiele kolumn w jeden wektor \textit{VectorAssembler} (z wytrenowanego pełnego potoku), wytrenowanego modelu \textit{RandomForestClassifier} oraz, co istotne, transformatora \textit{IndexToString}. Zadaniem \textit{IndexToString} jest przekształcenie numerycznych predykcji modelu (kolumna \textit{prediction}) z powrotem na etykiety tekstowe (kolumna \textit{predicted\_status}), wykorzystując mapowania (etykiety) nauczone przez \textit{StringIndexer} podczas treningu i zapisane w pełnym potoku. Dzięki temu aplikacja Scala otrzymuje predykcje w czytelnej, tekstowej formie. Skrypt treningowy posiada również logikę umożliwiającą wczytanie wcześniej zapisanego pełnego potoku deweloperskiego, jeśli jest on dostępny, co przyspiesza iteracyjne uruchomienia i pozwala na ponowne wygenerowanie modelu inferencyjnego bez pełnego treningu.

    \item \textbf{Wyniki treningu i skuteczność modelu}

Model przetestowano wstępnie na zbiorze testowym w zadanym przedziale czasowym dla poszczególnego urządzenia (w tym przypadku kompresora), wyniki przedstawione są na rysunkach~\ref{Wykres pokazujący prawdziwe stany urządzenia} oraz~\ref{Dane pokazujące sklasyfikowane stany urządzenia}. Jak pokazano na porównaniu model wykazał się dobrym poziomem skuteczności. Model został przetestowany na zbiorze treningowym, walidacyjnym i testowym przy użyciu metryki dokładności za pomocą ewaluatora \textit{MulticlassClassificationEvaluator}. 


\singlesizedimageforced{images/oryginal_klasyfikacja.png}{Wykres pokazujący prawdziwe stany urządzenia}{1.0}
\label{fig:prawdziwe_stany}

\singlesizedimageforced{images/klasyfikacja.png}{Dane pokazujące sklasyfikowane stany urządzenia}{1.0}
\label{fig:sklasyfikowane_stany}

    \item \textbf{Ewaluacja i wyniki}

Skuteczność wytrenowanego modelu jest weryfikowana na zbiorach treningowym, walidacyjnym i testowym przy użyciu metryki dokładności (accuracy) za pomocą ewaluatora \\ \textit{MulticlassClassificationEvaluator}.

W opracowanym systemie kluczową rolę odgrywa serializacja danych do formatu Apache Avro oraz integracja z usługą Confluent Schema Registry w celu zarządzania schematami. Elementem tej integracji, stosowanym przez wszystkie moduły przetwarzające dane \mbox{\textit{Apache Avro}}, jest specyficzny dla projektu mechanizm obsługi nagłówków. Przy konsumpcji danych z brokera \mbox{\textit{{Apache Kafka}}}, przychodzące komunikaty \mbox{\textit{Apache Avro}} mają usuwany 6-bajtowy nagłówek (składający się z tzw. magic byte oraz ID schematu) za pomocą operacji \textit{substring(value, 6)}. Natomiast przy wysyłaniu przetworzonych danych z powrotem do brokera \mbox{\text{Apache Kafka}}, odpowiedni nagłówek jest dodawany przed serializacją przez funkcję zdefiniowaną przez użytkownika (UDF, ang. \mbox{User-Defined Function})~\cite{spark_udf} o nazwie \textit{addSchemaRegistryHeaderUDF}. Takie podejście jest niestandardowym obejściem, koniecznym dla zapewnienia poprawnej współpracy aplikacji \mbox{\textit{Spark Structured Streaming}} ze schematami \mbox{\textit{Apache Avro}} zarządzanymi przez usługę \mbox{\textit{Confluent Schema Registry}}~\cite{confluent_schema_registry} w ramach tego projektu. Aplikacja korzysta również ze standardowych funkcji \mbox{\textit{Apache Spark}}, takich jak: konfiguracja lokalizacji punktów kontrolnych (\textit{spark.sql.streaming.checkpointLocation}) dla zapewnienia odporności na błędy w przetwarzaniu strumieniowym oraz możliwości ustawiania poziomu logowania. Dla celów diagnostycznych, niektóre procesory mogą, w zależności od konfiguracji, wypisywać przetwarzane dane również na konsolę. Źródłem danych wejściowych dla opracowanego systemu są komunikaty brokera \mbox{\textit{Apache Kafka}} generowane przez symulator (opisany w rozdziale~\ref{sec:implementacja_generowania}), naśladujący pracę rzeczywistych urządzeń przemysłowych.

\end{enumerate}
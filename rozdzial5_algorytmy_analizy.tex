\section{Algorytmy analizy danych}
\label{sec:algorytmy_analizy}

W niniejszym rozdziale omówiono algorytmy wykorzystane do analizy danych, zaimplementowane w ramach opracowanego systemu. System ten, nazwany \texttt{SparkDataProcessor}, został zrealizowany w języku Scala z wykorzystaniem frameworka Apache Spark. Jego głównym zadaniem jest przetwarzanie strumieni danych sensorycznych pochodzących z symulowanych urządzeń przemysłowych, takich jak pompy, sprężarki i turbiny, które są szczegółowo opisane w Rozdziale \ref{sec:implementacja_generowania}. Aplikacja analizuje cztery podstawowe typy pomiarów: ciśnienie, temperaturę, wilgotność oraz wibracje.

Architektura \texttt{SparkDataProcessor} opiera się na głównym obiekcie \texttt{SparkDataProcessor}, który inicjalizuje i zarządza poszczególnymi zadaniami przetwarzania strumieniowego. Działanie aplikacji jest w pełni konfigurowalne za pomocą pliku \texttt{application.conf} w formacie HOCON, który ładowany jest przy użyciu biblioteki PureConfig poprzez obiekt \texttt{AppConfig}. Konfiguracja ta pozwala na dynamiczne włączanie i wyłączanie poszczególnych modułów analitycznych oraz precyzyjne definiowanie ich parametrów, takich jak adresy serwerów Kafka, nazwy tematów wejściowych i wyjściowych, rozmiary okien czasowych dla agregacji, progi alarmowe dla detekcji zdarzeń, a także flagi debugowania.

Podstawą przetwarzanych informacji są modele danych zdefiniowane jako klasy przypadku (case classes) w języku Scala, znajdujące się w pakiecie \texttt{com.factory.model}. Reprezentują one odczyty z poszczególnych sensorów (np. \texttt{Pressure}, \texttt{Temperature}, \texttt{Humidity}, \texttt{Vibration}) oraz strukturę wykrytych zdarzeń (\texttt{Event}).

System \texttt{SparkDataProcessor} składa się z trzech głównych modułów przetwarzających dane:

Pierwszym z nich jest \texttt{MeanProcessor}, odpowiedzialny za obliczanie średnich wartości dla poszczególnych typów sensorów w zdefiniowanych oknach czasowych. Przetwarzanie to odbywa się dla każdego typu urządzenia (np. "pump", "compressor", "turbine"), identyfikowanego za pomocą etykiet (labels) w danych wejściowych. Moduł ten konsumuje dane w formacie Apache Avro z dedykowanych tematów Kafka. \texttt{MeanProcessor} wykorzystuje mechanizmy Spark Structured Streaming, w tym funkcje okienkowania (np. \texttt{window(col("event\_time"), "2 minutes", "1 minutes")}) oraz znaki wodne (watermarks) do obsługi danych opóźnionych. Zagregowane wartości średnie, wraz z liczbą odczytów, są następnie serializowane z powrotem do formatu Avro i publikowane na odpowiednie tematy Kafka (np. \texttt{pressureMean}).

Drugi moduł, \texttt{EventProcessor}, ma za zadanie monitorowanie strumieni danych sensorycznych pod kątem przekroczenia zdefiniowanych progów ostrzegawczych i krytycznych. Analiza ta jest prowadzona indywidualnie dla każdego typu sensora i etykiety urządzenia. Ten komponent wykorzystuje zaawansowaną operację \texttt{flatMapGroupsWithState} dostępną w Spark Structured Streaming, która pozwala na utrzymanie stanu dla każdego klucza (czyli dla każdej etykiety sensora). Moduł ten implementuje mechanizm "cooldown", zapobiegający generowaniu nadmiernej liczby alertów dla tego samego problemu w krótkim czasie. Okres wyciszenia może być zdefiniowany globalnie lub specyficznie dla danego typu sensora. Wykryte zdarzenia, zawierające m.in. tytuł, znacznik czasowy oraz status alertu (ostrzeżenie/krytyczny), są serializowane do formatu Avro przy użyciu schematu pobranego z Confluent Schema Registry i wysyłane do wspólnego tematu Kafka o nazwie \texttt{events}.

Trzecim, najbardziej złożonym modułem, jest \texttt{EquipmentStateProcessor}. Jego celem jest predykcja ogólnego stanu technicznego monitorowanego urządzenia na podstawie połączonych danych z wielu sensorów. Moduł ten subskrybuje dane Avro z czterech głównych tematów Kafki odpowiadających strumieniom ciśnienia, temperatury, wilgotności i wibracji. Każdy ze strumieni jest opatrzony znakiem wodnym w celu poprawnej obsługi danych napływających z opóźnieniem. Następnie strumienie te są łączone (join) na podstawie wspólnego klucza, który, zgodnie z dostarczonymi informacjami, reprezentuje identyfikator cyklu pomiarowego (kombinacja znacznika czasowego i typu urządzenia). Pozwala to na skorelowanie odczytów z różnych sensorów dla tego samego momentu i urządzenia. Połączone dane, zawierające cechy takie jak typ urządzenia (\texttt{equipment\_type}), znacznik czasowy zdarzenia (\texttt{event\_timestamp}) oraz wartości poszczególnych pomiarów, są przekazywane do wytrenowanego wcześniej modelu uczenia maszynowego. Model ten, będący klasyfikatorem \texttt{RandomForestClassifier} z biblioteki Spark ML, został wcześniej wytrenowany oraz został ładowany do aplikacji Sparka. Na podstawie tych danych model klasyfikuje stan urządzenia. Możliwe stany, szczegółowo opisane w Rozdziale \ref{sec:implementacja_generowania}, obejmują: "Stan normalny", "Wczesne zużycie", "Stan podkrytyczny", "Stan krytyczny" oraz "Naprawa". Oryginalne dane sensoryczne są następnie wzbogacane o tę przewidzianą etykietę stanu (\texttt{predicted\_status}). Tak przygotowane, wzbogacone dane dla każdego typu sensora (np. dane o ciśnieniu wraz z przewidywanym stanem) są serializowane do formatu Avro, z wykorzystaniem odpowiednich schematów (np. \texttt{pressureAugumented-value}) pobieranych z Confluent Schema Registry, i publikowane na nowe, dedykowane tematy Kafka (np. \texttt{pressureAugumented}). Głównym zastosowaniem tych wzbogaconych danych jest ich wizualizacja na wykresach w interfejsie użytkownika.

\subsection{Proces uczenia modelu predykcji stanu urządzenia}
\label{sec:uczenie_modelu_stanu}

Model klasyfikacji \texttt{RandomForestClassifier}, wykorzystywany przez moduł \texttt{EquipmentStateProcessor} do predykcji stanu technicznego urządzeń, jest trenowany w osobnym procesie wsadowym przy użyciu dedykowanego skryptu Python z biblioteką PySpark. Proces ten można podzielić na kilka kluczowych etapów.

\textbf{Przygotowanie danych.}
Źródłem danych treningowych są pliki CSV generowane przez symulator. Zawierają one historyczne odczyty sensorów (temperatura, ciśnienie, wibracje, wilgotność) oraz odpowiadający im rzeczywisty stan urządzenia (np. "Stan normalny", "Wczesne zużycie", itd.). Dane dla różnych typów urządzeń są wczytywane do Spark DataFrame, a następnie łączone w jeden zbiór. Przeprowadzane jest czyszczenie danych, które obejmuje m.in. usunięcie rekordów ze stanem \texttt{repair}, ponieważ ten stan nie jest przedmiotem predykcji w czasie rzeczywistym, lecz wynikiem działania serwisowego. Kluczowym krokiem jest konwersja kolumny znacznika czasu (np. \texttt{timestamp}) do odpowiedniego formatu Spark TimestampType i sortowanie danych chronologicznie. Cały zbiór danych jest następnie dzielony na zbiory: treningowy (70\% danych), walidacyjny (15\%) oraz testowy (15\%). Podział ten realizowany jest w oparciu o kolejność czasową rekordów, co uzyskuje się przez dodanie kolumny z monotonicznie rosnącym ID po sortowaniu (\texttt{monotonically\_increasing\_id()}) i filtrowanie na podstawie progów wyliczonych z proporcji. Taki sposób podziału ma na celu lepsze symulowanie rzeczywistych warunków predykcji przyszłych stanów na podstawie danych historycznych.

\textbf{Inżynieria cech.}
W tym etapie przygotowywane są cechy wejściowe dla modelu. Kolumny reprezentujące odczyty sensorów (\texttt{temperature}, \texttt{pressure}, \texttt{vibration}, \texttt{humidity}) są rzutowane na typ \texttt{double}. Zmienna docelowa, czyli kolumna \texttt{status} opisująca stan urządzenia, jest konwertowana na wartości numeryczne przy użyciu transformatora \texttt{StringIndexer}, tworząc nową kolumnę \texttt{label}. Etykiety tekstowe, które \texttt{StringIndexer} mapuje na indeksy, są zapamiętywane na potrzeby późniejszej konwersji predykcji z powrotem na tekst. Następnie, wybrane cechy numeryczne są łączone w jeden wektor cech za pomocą transformatora \texttt{VectorAssembler}, który tworzy kolumnę \texttt{features}.

\textbf{Definicja i trening modelu.}
Jako model klasyfikacyjny wykorzystywany jest \texttt{RandomForestClassifier} z biblioteki Spark MLlib, skonfigurowany m.in. z liczbą drzew decyzyjnych ustawioną na 100 (\texttt{numTrees=100}) oraz stałym ziarnem losowości (\texttt{seed=42}) dla zapewnienia reprodukowalności wyników. Cały proces transformacji danych i treningu modelu jest zdefiniowany jako potok (pipeline) Spark ML, składający się z sekwencji kroków: \texttt{StringIndexer} (dla kolumny \texttt{status}), \texttt{VectorAssembler} (dla cech wejściowych) oraz \texttt{RandomForestClassifier}. Model jest trenowany poprzez wywołanie metody \texttt{fit()} na tym potoku, używając przygotowanego wcześniej zbioru treningowego.

\textbf{Zapis i przygotowanie modelu do inferencji.}
Po zakończeniu treningu, wytrenowany potok jest zapisywany na dysku. Skrypt treningowy generuje dwie główne struktury modelu. Pierwsza, określana jako "pełny potok deweloperski" (zapisywana w ścieżce \texttt{spark\_model\_path + "\_dev\_full\_pipeline"}), zawiera wszystkie etapy, łącznie ze \texttt{StringIndexer}, i jest używana na potrzeby ewaluacji i dalszych prac deweloperskich w środowisku Python. Druga wersja, kluczowa dla aplikacji \texttt{SparkDataProcessor}, jest zapisywana bezpośrednio pod ścieżką docelową dla modelu inferencyjnego. Ten model inferencyjny jest specjalnie przygotowanym potokiem, który składa się z \texttt{VectorAssembler} (z wytrenowanego pełnego potoku), wytrenowanego modelu \texttt{RandomForestClassifier} oraz, co istotne, transformatora \texttt{IndexToString}. Zadaniem \texttt{IndexToString} jest przekształcenie numerycznych predykcji modelu (kolumna \texttt{prediction}) z powrotem na etykiety tekstowe (kolumna \texttt{predicted\_status}), wykorzystując mapowania (etykiety) nauczone przez \texttt{StringIndexer} podczas treningu i zapisane w pełnym potoku. Dzięki temu aplikacja Scala otrzymuje predykcje w czytelnej, tekstowej formie. Skrypt treningowy posiada również logikę umożliwiającą wczytanie wcześniej zapisanego pełnego potoku deweloperskiego, jeśli jest on dostępny, co przyspiesza iteracyjne uruchomienia i pozwala na ponowne wygenerowanie modelu inferencyjnego bez pełnego treningu.


DODAJ PORÓWNANIE ORYGINALNEJ CSV-KI I WYGENEROWANEJ PRZEZ MODEL

\textbf{Ewaluacja i wyniki.}
Skuteczność wytrenowanego modelu jest weryfikowana na zbiorach treningowym, walidacyjnym i testowym przy użyciu metryki dokładności (accuracy) za pomocą ewaluatora \texttt{MulticlassClassificationEvaluator}. 

W całym systemie \texttt{SparkDataProcessor} kluczową rolę odgrywa serializacja danych do formatu Apache Avro oraz integracja z Confluent Schema Registry w celu zarządzania schematami. Elementem tej integracji, stosowanym przez wszystkie moduły przetwarzające dane Avro, jest specyficzny dla projektu mechanizm obsługi nagłówków. Przy konsumpcji danych z Kafki, przychodzące komunikaty Avro mają usuwany 6-bajtowy nagłówek (składający się z tzw. magic byte oraz ID schematu) za pomocą operacji \texttt{substring(value, 6)}. Natomiast przy wysyłaniu przetworzonych danych z powrotem do Kafki, odpowiedni nagłówek jest dodawany przed serializacją przez funkcję zdefiniowaną przez użytkownika (UDF) o nazwie \texttt{addSchemaRegistryHeaderUDF}. Takie podejście jest niestandardowym obejściem, koniecznym dla zapewnienia poprawnej współpracy aplikacji Spark Structured Streaming ze schematami Avro zarządzanymi przez Confluent Schema Registry w ramach tego projektu. Aplikacja korzysta również ze standardowych funkcji Apache Spark, takich jak konfiguracja lokalizacji punktów kontrolnych (\texttt{spark.sql.streaming.checkpointLocation}) dla zapewnienia odporności na błędy w przetwarzaniu strumieniowym, oraz możliwości ustawiania poziomu logowania (np. \texttt{spark.sparkContext.setLogLevel("WARN")}). Dla celów diagnostycznych, niektóre procesory mogą, w zależności od konfiguracji, wypisywać przetwarzane dane również na konsolę. Źródłem danych wejściowych dla systemu są komunikaty Kafka generowane przez symulator opisany w Rozdziale \ref{sec:implementacja_generowania}, który naśladuje pracę rzeczywistych urządzeń przemysłowych.


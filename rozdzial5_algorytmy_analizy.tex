\section{Algorytmy analizy danych}
\label{sec:algorytmy_analizy}

W niniejszym rozdziale omówiono algorytmy wykorzystane do analizy danych, zaimplementowane w ramach opracowanego systemu. System ten, nazwany \texttt{SparkDataProcessor}, został zrealizowany w języku Scala z wykorzystaniem frameworka Apache Spark. Jego głównym zadaniem jest przetwarzanie strumieni danych sensorycznych pochodzących z symulowanych urządzeń przemysłowych, takich jak pompy, sprężarki i turbiny, które są szczegółowo opisane w Rozdziale \ref{sec:implementacja_generowania}. Aplikacja analizuje cztery podstawowe typy pomiarów: ciśnienie, temperaturę, wilgotność oraz wibracje.

Architektura \texttt{SparkDataProcessor} opiera się na głównym obiekcie \texttt{SparkDataProcessor}, który inicjalizuje i zarządza poszczególnymi zadaniami przetwarzania strumieniowego. Działanie aplikacji jest w pełni konfigurowalne za pomocą pliku \texttt{application.conf} w formacie HOCON, który ładowany jest przy użyciu biblioteki PureConfig poprzez obiekt \texttt{AppConfig}. Konfiguracja ta pozwala na dynamiczne włączanie i wyłączanie poszczególnych modułów analitycznych oraz precyzyjne definiowanie ich parametrów, takich jak adresy serwerów Kafka, nazwy tematów wejściowych i wyjściowych, rozmiary okien czasowych dla agregacji, progi alarmowe dla detekcji zdarzeń, a także flagi debugowania.

Podstawą przetwarzanych informacji są modele danych zdefiniowane jako klasy przypadku (case classes) w języku Scala, znajdujące się w pakiecie \texttt{com.factory.model}. Reprezentują one odczyty z poszczególnych sensorów (np. \texttt{Pressure}, \texttt{Temperature}, \texttt{Humidity}, \texttt{Vibration}) oraz strukturę wykrytych zdarzeń (\texttt{Event}).

System \texttt{SparkDataProcessor} składa się z trzech głównych modułów przetwarzających dane:

Pierwszym z nich jest \texttt{MeanProcessor}, odpowiedzialny za obliczanie średnich wartości dla poszczególnych typów sensorów w zdefiniowanych oknach czasowych. Przetwarzanie to odbywa się dla każdego typu urządzenia (np. "pump", "compressor", "turbine"), identyfikowanego za pomocą etykiet (labels) w danych wejściowych. Moduł ten konsumuje dane w formacie Apache Avro z dedykowanych tematów Kafka. \texttt{MeanProcessor} wykorzystuje mechanizmy Spark Structured Streaming, w tym funkcje okienkowania (np. \texttt{window(col("event\_time"), "2 minutes", "1 minutes")}) oraz znaki wodne (watermarks) do obsługi danych opóźnionych. Zagregowane wartości średnie, wraz z liczbą odczytów, są następnie serializowane z powrotem do formatu Avro i publikowane na odpowiednie tematy Kafka (np. \texttt{pressureMean}).

Drugi moduł, \texttt{EventProcessor}, ma za zadanie monitorowanie strumieni danych sensorycznych pod kątem przekroczenia zdefiniowanych progów ostrzegawczych i krytycznych. Analiza ta jest prowadzona indywidualnie dla każdego typu sensora i etykiety urządzenia. Ten komponent wykorzystuje zaawansowaną operację \texttt{flatMapGroupsWithState} dostępną w Spark Structured Streaming, która pozwala na utrzymanie stanu dla każdego klucza (czyli dla każdej etykiety sensora). Moduł ten implementuje mechanizm "cooldown", zapobiegający generowaniu nadmiernej liczby alertów dla tego samego problemu w krótkim czasie. Okres wyciszenia może być zdefiniowany globalnie lub specyficznie dla danego typu sensora. Wykryte zdarzenia, zawierające m.in. tytuł, znacznik czasowy oraz status alertu (ostrzeżenie/krytyczny), są serializowane do formatu Avro przy użyciu schematu pobranego z Confluent Schema Registry i wysyłane do wspólnego tematu Kafka o nazwie \texttt{events}.

Trzecim, najbardziej złożonym modułem, jest \texttt{EquipmentStateProcessor}. Jego celem jest predykcja ogólnego stanu technicznego monitorowanego urządzenia na podstawie połączonych danych z wielu sensorów. Moduł ten subskrybuje dane Avro z czterech głównych tematów Kafki odpowiadających strumieniom ciśnienia, temperatury, wilgotności i wibracji. Każdy ze strumieni jest opatrzony znakiem wodnym w celu poprawnej obsługi danych napływających z opóźnieniem. Następnie strumienie te są łączone (join) na podstawie wspólnego klucza, który, zgodnie z dostarczonymi informacjami, reprezentuje identyfikator cyklu pomiarowego (kombinacja znacznika czasowego i typu urządzenia). Pozwala to na skorelowanie odczytów z różnych sensorów dla tego samego momentu i urządzenia. Połączone dane, zawierające cechy takie jak typ urządzenia (\texttt{equipment\_type}), znacznik czasowy zdarzenia (\texttt{event\_timestamp}) oraz wartości poszczególnych pomiarów, są przekazywane do wytrenowanego wcześniej modelu uczenia maszynowego. Model ten, będący klasyfikatorem \texttt{RandomForestClassifier} z biblioteki Spark ML, został wcześniej wytrenowany oraz został ładowany do aplikacji Sparka. Na podstawie tych danych model klasyfikuje stan urządzenia. Możliwe stany, szczegółowo opisane w Rozdziale \ref{sec:implementacja_generowania}, obejmują: "Stan normalny", "Wczesne zużycie", "Stan podkrytyczny", "Stan krytyczny" oraz "Naprawa". Oryginalne dane sensoryczne są następnie wzbogacane o tę przewidzianą etykietę stanu (\texttt{predicted\_status}). Tak przygotowane, wzbogacone dane dla każdego typu sensora (np. dane o ciśnieniu wraz z przewidywanym stanem) są serializowane do formatu Avro, z wykorzystaniem odpowiednich schematów (np. \texttt{pressureAugumented-value}) pobieranych z Confluent Schema Registry, i publikowane na nowe, dedykowane tematy Kafka (np. \texttt{pressureAugumented}). Głównym zastosowaniem tych wzbogaconych danych jest ich wizualizacja na wykresach w interfejsie użytkownika.

W całym systemie \texttt{SparkDataProcessor} kluczową rolę odgrywa serializacja danych do formatu Apache Avro oraz integracja z Confluent Schema Registry w celu zarządzania schematami. Elementem tej integracji, stosowanym przez wszystkie moduły przetwarzające dane Avro, jest specyficzny dla projektu mechanizm obsługi nagłówków. Przy konsumpcji danych z Kafki, przychodzące komunikaty Avro mają usuwany 6-bajtowy nagłówek (składający się z tzw. magic byte oraz ID schematu) za pomocą operacji \texttt{substring(value, 6)}. Natomiast przy wysyłaniu przetworzonych danych z powrotem do Kafki, odpowiedni nagłówek jest dodawany przed serializacją przez funkcję zdefiniowaną przez użytkownika (UDF) o nazwie \texttt{addSchemaRegistryHeaderUDF}. Takie podejście jest niestandardowym obejściem, koniecznym dla zapewnienia poprawnej współpracy aplikacji Spark Structured Streaming ze schematami Avro zarządzanymi przez Confluent Schema Registry w ramach tego projektu. Aplikacja korzysta również ze standardowych funkcji Apache Spark, takich jak konfiguracja lokalizacji punktów kontrolnych (\texttt{spark.sql.streaming.checkpointLocation}) dla zapewnienia odporności na błędy w przetwarzaniu strumieniowym, oraz możliwości ustawiania poziomu logowania (np. \texttt{spark.sparkContext.setLogLevel("WARN")}). Dla celów diagnostycznych, niektóre procesory mogą, w zależności od konfiguracji, wypisywać przetwarzane dane również na konsolę. Źródłem danych wejściowych dla systemu są komunikaty Kafka generowane przez symulator opisany w Rozdziale \ref{sec:implementacja_generowania}, który naśladuje pracę rzeczywistych urządzeń przemysłowych.


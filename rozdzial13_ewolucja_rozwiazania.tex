\section{Ewolucja rozwiązania}
\label{chap:ewolucja_rozwiazania}
\addcontentsline{lof}{section}{Rozdział \ref{chap:ewolucja_rozwiazania}}
\addcontentsline{lot}{section}{Rozdział \ref{chap:ewolucja_rozwiazania}}
\addcontentsline{lol}{section}{Rozdział \ref{chap:ewolucja_rozwiazania}}


W trakcie realizacji niniejszego projektu będącego tematem niniejszej pracy, opracowywany system do analizy danych w czasie rzeczywistym przechodził przez kilka kluczowych faz ewolucyjnych. Pierwotne założenia i wybrane technologie były weryfikowane i często zastępowane przez bardziej efektywne rozwiązania. W niniejszym rozdziale przedstawiono najważniejsze zmiany w architekturze i implementacji, uzasadniając podjęte decyzje oraz analizując korzyści i koszty poszczególnych podejść.

\subsection{System autoryzacji}

Początkowa wersja opracowanego systemu opierała się na własnoręcznie zaimplementowanym mechanizmie autoryzacji w ramach dedykowanego mikroserwisu oraz bazy danych. Rozwiązanie to generowało szereg wyzwań. Zarządzanie cyklem życia tokenów JWT \cite{jwt_rfc}, w tym ich odświeżanie (ang. refresh tokens), było skomplikowane i podatne na błędy. Proces tworzenia i zarządzania użytkownikami oraz ich rolami wymagał implementacji dodatkowych endpointów REST API i interfejsów, co zwiększało nakład pracy. Mechanizmy bezpieczeństwa, takie jak ochrona przed atakami siłowymi (ang. brute force) czy zaawansowane polityki haseł, musiałyby być implementowane od podstaw.

Pierwotne rozwiązanie autoryzacji wymagało przechowywania danych użytkowników w bazie danych, następne mikroserwis UserService korzystał z wspomnianej bazy danych w celu tworzenia, logowania i aktualizacji danych użytkowników oraz zarządzania ich rolami. W celu generowania tokenów JWT \cite{jwt_rfc} używano biblioteki JJWT \cite{jjwt_docs}. W celu zarządzania cyklem życia tokenów JWT \cite{jwt_rfc}, w tym ich odświeżania, implementowano własny mechanizm. Problemem tutaj był jednak przymus implementacji mechanizmu bezpieczeństwa oraz wszystkich funkcjonalności, które serwer Keycloak ma wbudowane.

\vspace{0.3em}

W miarę rozwoju projektu podjęto decyzję o migracji na serwer Keycloak \cite{keycloak_docs}, co przyniosło następujące korzyści:
\begin{itemize}
    \item \textbf{standaryzacja i bezpieczeństwo} - serwer Keycloak \cite{keycloak_docs} dostarcza gotowe, przetestowane i zgodne ze standardami (np. OAuth 2.0 \cite{oauth2_rfc}) mechanizmy uwierzytelniania i autoryzacji, co znacząco podnosi poziom bezpieczeństwa systemu,
    \item \textbf{centralizacja zarządzania tożsamością} - zarządzanie użytkownikami, rolami i uprawnieniami odbywa się w jednym, dedykowanym do tego panelu administracyjnym, co upraszcza administrację oraz ułatwia zarządzanie użytkownikami,
    \item \textbf{elastyczność i skalowalność} - serwer Keycloak \cite{keycloak_docs} oferuje zaawansowane funkcje, takie jak: integracja z zewnętrznymi dostawcami tożsamości (np. LDAP \cite{ldap_rfc}, media społecznościowe), co otwiera system na przyszłe rozszerzenia.
\end{itemize}


Koszt tej zmiany był relatywnie niski i sprowadzał się głównie do nakładu pracy na rekonfigurację usług back-endowych i front-endu w celu integracji z serwerem Keycloak \cite{keycloak_docs}. Migracja na serwer Keycloak \cite{keycloak_docs} spowodowała przyspieszenie prac deweloperskich i testowych oraz poprawiło wygodę w zarządzaniu użytkownikami dzięki specjalistycznemu panelu administracyjnemu. Co najważniejsze, użyto rozwiazania sprawdzonego na rynku, co poprawiło bezpieczeństwo systemu będącego przedmiotem tej pracy.

\subsection{Infrastruktura klastra}

Pierwotnym założeniem było wdrożenie opracowanego systemu na zarządzanym przez \textbf{AWS} \cite{aws_docs} klastrze \textbf{Kubernetes} \cite{kubernetes}, czyli usłudze \textbf{EKS} (\mbox{Elastic Kubernetes Service}) \cite{eks_docs}. Główne zalety tego podejścia to uproszczone zarządzanie oraz łatwa integracja z innymi usługami \textbf{AWS} \cite{aws_docs}. Jednakże, analiza kosztów wykazała, że utrzymanie nawet minimalnego klastra \textbf{EKS} \cite{eks_docs} generuje stałe, znaczące opłaty, co w przypadku projektu badawczo-rozwojowego było nieuzasadnione ekonomicznie.

Alternatywną próbą było ręczne postawienie klastra na maszynach wirtualnych \textbf{EC2} \\ (\mbox{Amazon Elastic Compute Cloud} - usługa instancji wirtualnych) \cite{ec2_docs}. Podejście to, choć tańsze, wiązało się z ogromną złożonością konfiguracyjną, długim czasem wdrażania i choć jest tańsze niż \textbf{EKS} \cite{eks_docs}, to jednak i tak generowało zbyt wiele kosztów. W tym przypadku wymagane jest wdrażanie całego klastra oraz aplikacji za pomocą skryptów \textbf{AWS CloudFormation} \cite{cloudformation_docs} w celu zmniejszenia kosztów. \textbf{AWS CloudFormation} \cite{cloudformation_docs} jest narzędziem \textbf{AWS} \cite{aws_docs}, pozwalającym na deklaratywne definiowanie zasobów w postaci szablonów \textbf{YAML} (format plików konfiguracyjnych) \cite{yaml_spec}. Jako zasoby można rozumieć tutaj instancje \textbf{EC2} \cite{ec2_docs}, sieci VPC, konfiguracje zapory ogniowej, itp. Proces ten jednak trwa długo i jednorazowo zajmuje około 10 minut wraz ze skryptami inicjalizującymi instancje oraz instalującymi wymagane aplikacje na systemach Linux. Rozwiązanie oparte na maszynach wirtualnych \textbf{EC2} \cite{ec2_docs} wymagało nadzoru od strony administratora klastra w celu redukcji kosztów lub automatyzacji włączania i wyłączania maszyn wirtualnych w zależności od potrzeb. Jednak klaster by pełnić swoją rolę nie może mieć przerw operacyjnych. Na listingu \ref{lst:eks_cloudformation} przedstawiono przykładowy skrypt \textbf{AWS CloudFormation} \cite{cloudformation_docs} do wdrożenia klastra \textbf{EKS} \cite{eks_docs}. Oryginalny skrypt jest dużo bardziej skomplikowany, dlatego przedstawiono jego uproszczoną wersję.

\begin{lstlisting}[caption=Skrypt AWS CloudFormation do wdrożenia klastra EKS, label={lst:eks_cloudformation}]

AWSTemplateFormatVersion: '2010-09-09'

Parameters:
  KeyPairName:
    Type: AWS::EC2::KeyPair::KeyName
    Description: Name of an existing EC2 KeyPair to enable SSH access
    
  VpcId:
    Type: AWS::EC2::VPC::Id
    Description: VPC ID where instances will be deployed
    
  SubnetId:
    Type: AWS::EC2::Subnet::Id
    Description: Subnet ID where instances will be deployed

Resources:
  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for EC2 instances
      VpcId: !Ref VpcId
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0
          Description: SSH access
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          CidrIp: 0.0.0.0/0
          Description: HTTP access
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: 0.0.0.0/0
          Description: HTTPS access
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
          Description: All outbound traffic
      Tags:
        - Key: Name
          Value: Ubuntu-Instances-SG

  Master:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-0866a3c8686eaeeba  # Ubuntu 24.04 LTS 
      InstanceType: t3.medium
      KeyName: !Ref KeyPairName
      SubnetId: !Ref SubnetId
      SecurityGroupIds:
        - !Ref InstanceSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          apt-get update
          apt-get install -y awscli
          hostnamectl set-hostname ubuntu-instance-1
      Tags:
        - Key: Name
          Value: Ubuntu-Instance-1

  Node:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-0866a3c8686eaeeba  # Ubuntu 24.04 LTS
      InstanceType: t3.medium
      KeyName: !Ref KeyPairName
      SubnetId: !Ref SubnetId
      SecurityGroupIds:
        - !Ref InstanceSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          apt-get update
          apt-get install -y awscli
          hostnamectl set-hostname ubuntu-instance-2
      Tags:
        - Key: Name
          Value: Ubuntu-Instance-2

\end{lstlisting}

Rozwiązanie oparte na maszynach wirtualnych EC2 należało zmienić. Koszty oraz elastyczność wdrażania zmian w klastrze wymagały poprawy. Dlatego zakupiono dwa komputery serwerowe fizyczne i zainstalowano na nich klaster ręcznie. Krytyczna ocena tej decyzji wskazuje, że choć lokalna infrastruktura znacząco zredukowała koszty operacyjne, wprowadziła wyzwania związane z zarządzaniem sprzętem i ograniczoną elastycznością skalowania. W dłuższej perspektywie, rozwiązanie hybrydowe łączące lokalną infrastrukturę z chmurą mogłoby stanowić optymalny kompromis między kosztami a możliwościami skalowania.

Poniżej \ref{tab:koszty_rozwiazan_wdrazenia} zostały przedstawione koszty w zależności od rozwiązania. Koszty wyrażone są i obejmują tylko zasoby obliczeniowe, nie sieciowe. Koszty dla zasobów AWS zostały obliczone za pomocą tej strony: \url{https://calculator.aws/}. Koszty związane z zasobami AWS są comiesięczne, a koszt zakupu infrastruktury lokalnej jest jednorazowy. Należy jednak zastrzec, że stosunki kosztów będą się różnić od skali wdrożenia i dla wdrożeń wielkoskalowych rozwiązania oparte na EC2/EKS mogą być niższe.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|lll}
    \cline{1-2}
    Rozwiązanie & Koszt &  &  &  \\ \cline{1-2}
    EKS         & 476   &  &  &  \\ \cline{1-2}
    EC2         & 439   &  &  &  \\ \cline{1-2}
    Lokalne     & 1024  &  &  &  \\ \cline{1-2}
    \end{tabular}
    \caption{Koszty rozwiązań wdrożenia}
    \label{tab:koszty_rozwiazan_wdrazenia}
\end{table}

\vspace{0.3em}

Ostatecznie wybrano rozwiązanie lokalne – stworzenie klastra przy użyciu narzędzia \textbf{Kubeadm} \cite{kubeadm_docs} na własnych maszynach. Pozwoliło to na:
\begin{itemize}
    \item \textbf{znaczącą redukcję kosztów} w porównaniu do \textbf{EKS} \cite{eks_docs},
    \item \textbf{uproszczenie i automatyzację procesu tworzenia klastra} w porównaniu do w pełni manualnej konfiguracji na \textbf{EC2} \cite{ec2_docs},
    \item \textbf{pełną kontrolę nad konfiguracją klastra} przy jednoczesnym zachowaniu zgodności ze standardami \textbf{Kubernetes} \cite{kubernetes}.
\end{itemize}

\subsection{Przetwarzanie danych}

W pierwszej implementacji autorskiego systemu do przetwarzania danych strumieniowych wykorzystano bibliotekę \mbox{Kafka Streams} \cite{kafka_streams}. Zaletą tego rozwiązania była ścisła integracja z ekosystemem \mbox{Apache Kafka} \cite{kafka} oraz niskie opóźnienia dzięki przetwarzaniu rekord po rekordzie. Jednak w trakcie rozwoju pojawiły się następujące ograniczenia:
\begin{itemize}
    \item \textbf{Czytelność kodu} - Implementacja bardziej złożonej logiki biznesowej, zwłaszcza operacji stanowych i łączenia wielu strumieni, prowadziła do skomplikowanego i trudnego w utrzymaniu kodu. Kod zawierał dużą liczbę klas, trudnych do zrozumienia i utrzymania.
    \item \textbf{Integracja z uczeniem maszynowym} - Bezpośrednia integracja modeli uczenia maszynowego (np. z biblioteki \mbox{Spark MLlib} \cite{spark_mllib_reference}) w ramach aplikacji \mbox{Kafka Streams} nie jest możliwa w bezpośredni sposób i wymagałaby implementacji dodatkowych warstw komunikacji.
\end{itemize}

Poniżej \ref{lst:pressure_mean_stream_factory} przedstawiono przykładowy kod implementacji w klasie \texttt{PressureMeanStreamFactory}.

\begin{lstlisting}[caption=Implementacja w klasie PressureMeanStreamFactory, label={lst:pressure_mean_stream_factory},language=Java]

    public class PressureMeanStreamFactory extends MeanStreamFactory<Pressure, PressureAggregation> {

    @Builder
    public PressureMeanStreamFactory(final KafkaNativeConfig kafkaNativeConfig, final MeanStreamConfig config) {
        super(kafkaNativeConfig, config);
    }

    @Override
    protected Predicate[] preparePredicates() {
        return getLabels().stream()
                .map(label -> (Predicate<String, Pressure>) (key, value) -> 
                        value.getLabel().toString().equals(label))
                .toArray(Predicate[]::new);
    }

    public List<KStream<String, Pressure>> splitToMeanBranches(
        final KStream<String, Pressure> inputStream) {
        var result = splitToPredicatedBranches(inputStream);

        for (var pressureStream : result) {
            pressureStream
                    .groupBy((key, value) -> value.getLabel().toString(),
                            Grouped.with(Serdes.String(), getSerde()))
                    .windowedBy(getWindowing())
                    .aggregate(
                            () -> PressureAggregation.newBuilder()
                                    .setData(Pressure.newBuilder()
                                            .setTimestamp(ZonedDateTime
                                                    .now().toEpochSecond())
                                            .setLabel("")
                                            .setData(PressureDataRecord
                                                        .newBuilder()
                                                    .setPressure(INITIAL_VALUE)
                                                    .build())
                                            .build()
                                    )
                                    .setCount(0)
                                    .build(),
                            (key, value, aggregated) -> PressureAggregation
                                .newBuilder()
                                    .setData(Pressure.newBuilder()
                                            .setTimestamp(ZonedDateTime
                                                    .now().toEpochSecond())
                                            .setLabel(value.getLabel())
                                            .setData(PressureDataRecord
                                                .newBuilder()
                                                .setPressure(
                                                        value.getData()
                                                        .getPressure() 
                                                        + 
                                                        aggregated.getData()
                                                        .getData()
                                                        .getPressure())
                                                .build())
                                            .build())
                                    .setCount(aggregated.getCount() + 1)
                                    .build(),
                            Materialized.with(Serdes.String(), 
                                              getAggregateSerdes())
                    )
                    .suppress(Suppressed.untilWindowCloses(Suppressed
                                            .BufferConfig.unbounded()))
                    .toStream()
                    .map((key, value) -> KeyValue.pair(key.key(),
                            Pressure.newBuilder()
                                    .setTimestamp(ZonedDateTime.now()
                                                    .toEpochSecond())
                                    .setLabel(value.getData().getLabel())
                                    .setData(PressureDataRecord.newBuilder()
                                            .setPressure(value.getData()
                                            .getData().getPressure() 
                                            / 
                                            value.getCount())
                                            .build())
                                    .build())
                    )
                    .to((key, value, recordContext) -> value.getLabel().toString() + getOutputTopicsPostfix(),
                            Produced.with(Serdes.String(), getSerde()));

        }

        return result;
    }
}  

\end{lstlisting}

Powyższy kod przedstawia klasę \texttt{PressureMeanStreamFactory}, dziedziczącą po generycznej klasie \texttt{MeanStreamFactory} i służącą do przetwarzania strumieni danych ciśnienia za pomocą biblioteki Kafka Streams. Klasa ta implementuje logikę rozdzielania strumienia wejściowego na podstrumienie według typów urządzeń (sprężarka, turbina, kompresor, itp.), a następnie agreguje dane w oknach czasowych, licząc średnie wartości ciśnienia dla każdej etykiety. Wyniki są następnie publikowane z podziałem na odpowiednie tematy wyjściowe.

\vspace{0.3em}

Kod przedstawiony na listingu \ref{lst:pressure_mean_stream_factory} jest jednak trudny do czytania i utrzymania z kilku powodów:
\begin{itemize}
    \item zagnieżdżenie wielu wywołań metod sprawia, że pojedyncze operacje są rozciągnięte na wiele poziomów, co utrudnia zrozumienie przepływu danych,
    \item logika agregacji i mapowania jest rozbudowana i zawiera powtarzające się fragmenty kodu, co zwiększa ryzyko popełnienia błędów i utrudnia wprowadzanie zmian,
    \item użycie wielu anonimowych funkcji lambda oraz typów generycznych powoduje, że kod jest mało przejrzysty, szczególnie dla osób nieznających szczegółów działania Kafka Streams \cite{kafka_streams},
    \item brak wyodrębnienia poszczególnych etapów przetwarzania do osobnych metod lub klas powoduje, że cała złożona logika znajduje się w jednym miejscu, co utrudnia testowanie i ponowne wykorzystanie kodu.
\end{itemize}

W rozważanym kodzie klas dziedziczących po klasie \texttt{MeanStreamFactory} było kilka. Jego refaktoryzacja nie zwiększyła czytelności kodu, gdyż biblioteka Kafka Streams \cite{kafka_streams} operuje w większości na funkcjach anonimowych. Natomiast silnik Apache Spark oferuje bardziej czytelny i zrozumiały interfejs API, co pozwoliło na poprawę czytelności kodu. Na listingu \ref{lst:mean_processor_spark} przedstawiono przykładowy kod implementacji w klasie \texttt{MeanProcessor}.

\begin{lstlisting}[caption=Implementacja w klasie MeanProcessor za pomocą Apache Spark, label={lst:mean_processor_spark},language=Scala]

class MeanProcessor[T <: SensorReading : Encoder](
                                                   spark: SparkSession,
                                                   config: SensorConfig,
                                                   kafkaConfig: KafkaConfig,
                                                   sensorType: String
                                                 ) {
  private val logger = LoggerFactory.getLogger(getClass)


  private def addSchemaRegistryHeaderUDF(schemaId: Int) = udf((avroBytes: Array[Byte]) => {
    val result = new Array[Byte](1 + 4 + avroBytes.length)

    result(0) = 0

    val schemaIdBytes = ByteBuffer.allocate(4).putInt(schemaId).array()
    System.arraycopy(schemaIdBytes, 0, result, 1, 4)

    System.arraycopy(avroBytes, 0, result, 5, avroBytes.length)

    result
  })

  def start(): Unit = {
    import spark.implicits._

    logger.info(s"Starting mean calculation for $sensorType")
    val restService = new RestService(kafkaConfig.schemaRegistryUrl)

    val inputStream = spark
      .readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", kafkaConfig.bootstrapServers)
      .option("subscribe", config.inputTopic)
      .option("startingOffsets", "latest")
      .load()

    config.labels.foreach { label =>
      logger.warn(s"Setting up processing for $sensorType label: $label")

      val outputSchema = restService
                        .getLatestVersion(sensorType + "Mean" + "-value")
      val subjectPostfix = sensorType.substring(0, 1).toUpperCase 
                           + sensorType.substring(1)
      logger.warn(s"subjectPostfix $subjectPostfix")
      val jsonFormatSchema = restService
                .getLatestVersion("com.factory.message." + subjectPostfix)

      val valueDF = inputStream
        .withColumn("stripped_value", expr("substring(value, 6)"))
        .select(from_avro(data = $"stripped_value", jsonFormatSchema.getSchema)
        .as(s"$sensorType"))
        .filter(col(s"$sensorType.label") === label)
        .withColumn("event_time", from_unixtime(col(s"$sensorType.timestamp"))
        .cast("timestamp"))

      val aggregatedDS = valueDF
        .withWatermark("event_time", s"1 minutes")
        .groupBy(window(col("event_time"), 
                        s"2 minutes", s"1 minutes"), $"$sensorType.label")
        .agg(
          avg(col(s"$sensorType.data.$sensorType")).cast("float")
                                                   .as(s"$sensorType"),
          count("*").cast("int").as("count")
        )
        .filter(row => !row.isNullAt(1))
        .withColumn("label", lit(label))
        .withColumn("timestamp", unix_timestamp(col("window.end")))
        .filter(col("count") > 0)
        .select(
          struct(
            struct(
              col("label"),
              col("timestamp"),
              struct(
                col(s"$sensorType")
              ).as("data")
            ).as("data"),
            col("count")
          ).as("result"),
          col("timestamp")
        )
        .select(
          to_avro($"result", outputSchema.getSchema)
            .as("pre_value"),
          col("timestamp").cast("string").as("key")
        )
        .withColumn("value", addSchemaRegistryHeaderUDF(outputSchema.getId)(col("pre_value")))

      aggregatedDS
        .writeStream
        .format("kafka")
        .option("kafka.bootstrap.servers", kafkaConfig.bootstrapServers)
        .option("topic", s"${sensorType}Mean")
        .outputMode("append")
        .start()

    }
  }
} 

\end{lstlisting}

Analizując powyższy kod można zauważyć, że obsługuje on wszystkie typy czujników w ten sam sposób, dzięki czemu nie trzeba tworzyć nowych klas dla każdego typu czujnika.

Podjęto decyzję o przejściu na bibliotekę \textbf{\mbox{Apache Spark Structured Streaming}} \cite{spark_streaming}. Mimo że działa ona w modelu mikrowsadowym, wprowadzającym nieco większe opóźnienia, przyniósł ona kluczowe korzyści:
\begin{itemize}
    \item \textbf{wyższy poziom abstrakcji} - deklaratywny interfejs API silnika \mbox{Apache Spark}, podobny do SQL, znacząco uprościł implementację logiki przetwarzania danych i poprawił czytelność kodu,
    \item \textbf{natywna integracja z ML} - silnik \mbox{Apache Spark} oferuje bogaty ekosystem, w tym bibliotekę \mbox{Spark ML} \cite{spark_streaming}, co umożliwiło bezproblemowe włączenie wytrenowanego modelu \texttt{RandomForestClassifier} bezpośrednio do potoku przetwarzania strumieniowego,
    \item \textbf{mechanizm checkpointów} - silnik \mbox{Apache Spark} pozwala na zapisanie stanu przetwarzania do dysku, co pozwala na kontynuowanie pracy w przypadku awarii.
\end{itemize}

\newpage

\subsection{Model bazy danych}

Początkowy projekt relacyjnej bazy danych zakładał tworzenie osobnej tabeli dla każdego typu czujnika (np. temperatura, cisnienie, wibracje). Na listingu \ref{lst:first_form_table} przedstawiono sposób tworzenia kilku przykładowych tabel według takiego podejścia.

\begin{lstlisting}[caption=Pierwsza forma tabeli w relacyjnej bazie danych, label={lst:first_form_table},language=SQL]


CREATE TABLE IF NOT EXISTS mean_temperature
(
    id        uuid NOT NULL,
    value     double precision,
    label     text,
    event_key text,
    timestamp timestamp with time zone,
    PRIMARY KEY (id),
    UNIQUE (label, event_key)
);

CREATE TABLE IF NOT EXISTS mean_pressure
(
    id        uuid NOT NULL,
    value     double precision,
    label     text,
    event_key text,
    timestamp timestamp with time zone,
    PRIMARY KEY (id),
    UNIQUE (label, event_key)
);

CREATE TABLE IF NOT EXISTS mean_flow_rate
(
    id        uuid NOT NULL,
    value     double precision,
    label     text,
    event_key text,
    timestamp timestamp with time zone,
    PRIMARY KEY (id),
    UNIQUE (label, event_key)
);

\end{lstlisting}

\vspace{0.3em}

Podejście to, choć z początku wyglądające dość intuicyjnie, miało poważne wady:
\begin{itemize}
    \item \textbf{brak elastyczności} - dodanie nowego typu czujnika wymagało zmiany schematu bazy danych (dodania nowej tabeli) oraz modyfikacji kodu w serwisach aplikacyjnych,
    \item \textbf{skomplikowane zapytania} - analiza danych z wielu typów czujników jednocześnie wymagała skomplikowanych złączeń (operacji JOIN) \cite{kleppmann2017designing} między wieloma tabelami, co było nieefektywne.
\end{itemize}

\vspace{0.3em}

\newpage

Wobec powyższego zmienione zostało podejście do zadania tak, że cały model danych oparty został na jednej tabeli. Sposób jej tworzenia został przedstawiony na listingu \ref{lst:second_form_table}.

\begin{lstlisting}[caption=Druga forma tabeli w relacyjnej bazie danych, label={lst:second_form_table},language=SQL]

CREATE TABLE IF NOT EXISTS sensor_data
(
    id          uuid NOT NULL,
    sensor_type text NOT NULL,
    label       text,
    event_key   text,
    timestamp   timestamp with time zone,
    data        jsonb NOT NULL,
    PRIMARY KEY (id),
    UNIQUE (sensor_type, label ,event_key)
);
\end{lstlisting}


Nowy, zunifikowany model danych opiera się na jednej, centralnej tabeli \texttt{sensor\_data}, zawierającej kolumny takie jak: \texttt{sensor\_type}, \texttt{label}, \texttt{event\_key}, \texttt{timestamp}, \texttt{data}. Ta zmiana przyniosła fundamentalne korzyści:
\begin{itemize}
    \item \textbf{elastyczność} - system może teraz obsługiwać dowolną liczbę typów czujników bez jakichkolwiek zmian w strukturze bazy danych,
    \item \textbf{uproszczenie zapytań} - analiza danych sprowadza się do prostych zapytań zawartych w jednej tabeli z odpowiednim filtrowaniem i grupowaniem.
\end{itemize}

\subsection{Zarządzanie wdrożeniami}

Pierwotnie stosowano ręczne zarządzanie manifestami YAML \cite{yaml_spec} klastra Kubernetes \cite{kubernetes}. Każdy projekt aplikacji miał folder z własnymi manifestami. Skrypt ten, choć automatyzował część procesu, był proceduralny i wymagał implementacji logiki oczekiwania na gotowość poszczególnych zasobów (np. węzłów, usług, a nawet na konkretne zdarzenia z SNS \cite{sns_docs}). Na klastrze również istniała specjalna aplikacja, zarządzająca wdrażaniem nowych wersji aplikacji tzw. deployer. Narzędzie to nasłuchiwało kanał SNS \cite{sns_docs}. Programista musiał najpierw zaaktualizować repozytorium z aplikacją, później wysłać wiadomość na niniejszy kanał SNS \cite{sns_docs}. Następnie wspomniana aplikacja rozpoczynała proces wdrażania nowej wersji aplikacji. Cały proces pokazano na rysunku \ref{Proces wdrażania starej wersji aplikacji}.

\singlesizedimageforced{images/deployer-flow.png}{Proces wdrażania starej wersji aplikacji}{1.0}

Rozwiązanie to było jednak było mało elastyczne zaś aplikacja często się zawieszała. W celu poprawy stabilności użyto menedżera pakietów \textbf{Helm} \cite{helm_docs}, umożliwiającego deklaratywne definiowanie pożądanego stanu aplikacji w postaci szablonów (ang. charts). Następnie wszystkie szablony osadzono w jednym repozytorium, co pozwoliło na automatyczne wdrażanie nowych wersji aplikacji za pomocą jednej komendy. Stan repozytorium odzwierciedlał stan klastra Kubernetes \cite{kubernetes}.

\newpage

Wprowadzenie menedżera pakietów \textbf{Helm} \cite{helm_docs} uprościło proces wdrożenia. Główne zalety to:
\begin{itemize}
    \item \textbf{deklaratywne zarządzanie} - menedżer pakietów Helm \cite{helm_docs} pozwala na definiowanie pożądanego stanu aplikacji w postaci szablonów (ang. charts), a sam zarządza procesem osiągnięcia tego stanu,
    \item \textbf{reużywalność i parametryzacja} - stworzenie wspólnego, reużywalnego szablonu wdrożenia dla wszystkich mikroserwisów znacząco uprościło konfigurację i zapewniło spójność wdrożeń w całym systemie,
    \item \textbf{zarządzanie cyklem życia aplikacji} - menedżer pakietów Helm \cite{helm_docs} ułatwia procesy aktualizacji, wycofywania zmian (ang. rollback) oraz usuwania aplikacji z klastra.
\end{itemize}

\subsection{Środowisko uruchomieniowe kontenerów}

Początkowo klaster \textbf{Kubernetes} \cite{kubernetes} był oparty na środowisku kontenerowym \texttt{Docker} \cite{docker_docs}. Jednakże, ze względu na zmiany w ekosystemie \textbf{Kubernetes} \cite{kubernetes}, podjęto decyzję o przejściu na środowisko kontenerowe \texttt{Containerd} \cite{containerd_docs}. Jest to zgodne z obecnymi standardami i rekomendacjami dla klastra \textbf{Kubernetes} \cite{kubernetes}. \texttt{Containerd} \cite{containerd_docs}, będący lżejszym i bardziej wyspecjalizowanym środowiskiem uruchomieniowym, oferuje lepszą wydajność i mniejszy narzut zasobów w porównaniu do pełnego silnika \texttt{Docker} \cite{docker_docs}, co jest kluczowe w kontekście efektywnego zarządzania zasobami klastra.

\subsection{Architektura sieciowa}

Początkowo stosowana konfiguracja sieciowa, oparta na usługach typu \texttt{NodePort} \cite{nodeport_docs} i zewnętrznym balanserze \textbf{AWS Application Load Balancer} (\textbf{ALB}) \cite{alb_docs}, była prosta w implementacji, ale miała istotne wady. Ruch musiał przechodzić przez dodatkową warstwę w chmurze \textbf{AWS} \cite{aws_docs}, co generowało duże opóźnienia. Co więcej, takie podejście utrudniało zarządzanie ruchem wewnątrz klastra i nie pozwalało na elastyczne wykorzystanie nazw DNS dla poszczególnych usług.

Zmienono więc sposób dostępu do aplikacji na klastrze. Użyto balansera obciążenia ruchu sieciowego \texttt{MetalLB} \cite{metallb_docs} w trybie L2 do udostępniania zewnętrznych adresów IP w lokalnej sieci oraz kontroler \texttt{Nginx Ingress} \cite{nginx_ingress_docs} do zaawansowanego routingu ruchu \texttt{HTTP}/\texttt{HTTPS}. Balanser \texttt{MetalLB} \cite{metallb_docs} można zainstalować na klastrach \textbf{Kubernetes} \cite{kubernetes}, osadzonych na fizycznych maszynach. Dało to możliwość lokalnego rozwiązywania nazw DNS dla poszczególnych usług oraz sprawiło, że odpowiedź z klastra była szybsza.

Modyfikacja ta pozwoliła na:
\begin{itemize}
    \item \textbf{wyeliminowanie zależności od zewnętrznego balansera chmurowego}, co skróciło opóźnienia,
    \item \textbf{elastyczne zarządzanie routingiem} na podstawie nazw hostów (DNS), co umożliwiło łatwe udostępnianie wielu usług (np. front-service.local, keycloak.local) pod jednym adresem IP,
    \item \textbf{centralizację zarządzania certyfikatami \texttt{TLS}} dzięki integracji kontrolera Ingress z narzędziem \texttt{cert-manager} \cite{cert_manager_docs}.
\end{itemize}

\newpage

\subsection{Krytyczna analiza decyzji projektowych}

W niniejszej sekcji przedstawiono krytyczną analizę kluczowych decyzji projektowych i technologicznych podjętych w trakcie realizacji systemu. Każda decyzja została uzasadniona przez porównanie z alternatywnymi rozwiązaniami oraz analizę korzyści i kompromisów.

\subsubsection{Architektura systemu}

Wybór architektury mikroserwisowej zamiast tradycyjnej architektury monolitycznej był uzasadniony potrzebą lepszej skalowalności i odporności na awarie w środowiskach przemysłowych o wysokiej dostępności \cite{microservice_benefits}. Architektura monolityczna, choć prostsza w początkowej implementacji, utrudnia niezależne skalowanie poszczególnych komponentów i wprowadza ryzyko pojedynczego punktu awarii, co jest nieakceptowalne w krytycznych aplikacjach przemysłowych. Mikroserwisy umożliwiają niezależne wdrażanie, skalowanie i aktualizację poszczególnych funkcjonalności, co znacząco poprawia niezawodność systemu i umożliwia lepsze wykorzystanie zasobów infrastrukturalnych.

\subsubsection{Platforma orkiestracji}

Wybór klastra Kubernetes jako platformy orkiestracji zamiast alternatywnych rozwiązań jak orkiestratów \textbf{\mbox{Docker Swarm}} czy \textbf{\mbox{Apache Mesos}} był uzasadniony jego dojrzałością ekosystemu, bogatą funkcjonalnością oraz dominującą pozycją na rynku \cite{kubernetes_benefits}. Klaster Kubernetes oferuje bardziej zaawansowane mechanizmy skalowania, zarządzania zasobami i bezpieczeństwa niż orkiestrator \textbf{\mbox{Docker Swarm}}, choć wprowadza większą złożoność konfiguracyjną. Podobnie, wybór menedżera pakietów \textbf{\mbox{Helm}} \cite{helm_docs} zamiast ręcznego zarządzania manifestami \textbf{YAML} \cite{yaml_spec} znacząco uprościł proces wdrażania i zarządzania cyklem życia aplikacji, umożliwiając deklaratywne definiowanie pożądanego stanu systemu.

\subsubsection{Algorytmy uczenia maszynowego}

Wybór \textbf{\mbox{RandomForestClassifier}} zamiast innych algorytmów uczenia maszynowego, takich jak: sieci neuronowe czy algorytmy liniowe (np. \textbf{\mbox{SVM}}, regresja logistyczna), był uzasadniony jego odpornością na przeuczenie, dobrą interpretowalnością oraz zdolnością do obsługi danych o mieszanej naturze (numeryczne i kategoryczne) bez konieczności wstępnej normalizacji \cite{spark_mllib_reference}. Algorytmy liniowe, choć szybsze w treningu, gorzej radzą sobie z nieliniowymi zależnościami między parametrami sensorycznymi, podczas gdy sieci neuronowe wymagają większej ilości danych treningowych i są mniej interpretowalne w kontekście diagnostyki przemysłowej.

Krytyczna ocena skuteczności modelu wskazuje, że choć model osiąga akceptowalną skuteczność w warunkach kontrolowanych, jego praktyczna użyteczność wymagałaby dalszej walidacji na rzeczywistych danych przemysłowych oraz potencjalnego rozszerzenia o mechanizmy adaptacyjne, takie jak: uczenie nadzorowane czy uczenie zespołowe z większą liczbą modeli bazowych.

\subsubsection{Frameworki interfejsu użytkownika}

Wybór biblioteki React jako podstawy interfejsu użytkownika zamiast alternatywnych frameworków jak: \textbf{\mbox{Angular}} czy \textbf{\mbox{Vue.js}} był uzasadniony jej komponentowym podejściem, które umożliwia łatwą modularyzację i ponowne wykorzystanie komponentów. React oferuje również bogaty ekosystem bibliotek i narzędzi, co przyspiesza rozwój aplikacji. W porównaniu z frameworkiem \textbf{\mbox{Angular}}, bibliotek \textbf{\mbox{React}} ma niższą krzywą uczenia się i większą elastyczność w integracji z różnymi technologiami back-endowymi.

\subsubsection{Systemy autoryzacji}

Wybór serwera \textbf{\mbox{Keycloak}} \cite{keycloak_docs} zamiast rozwiązań komercyjnych jak: platforma \textbf{\mbox{Auth0}} czy własnych implementacji standardu tokenów \textbf{\mbox{JWT}} \cite{jwt_rfc} był uzasadniony jego otwartoźródłowym charakterem, bogatą funkcjonalnością oraz możliwością samodzielnego hostingu, co zapewnia pełną kontrolę nad bezpieczeństwem danych. W porównaniu z platformą \textbf{\mbox{Auth0}}, serwer \textbf{\mbox{Keycloak}} oferuje podobne możliwości autoryzacji, ale bez kosztów subskrypcji i z większą elastycznością w dostosowaniu do specyficznych wymagań bezpieczeństwa przemysłowego. Alternatywa własnej implementacji standardu tokenów \textbf{\mbox{JWT}}, choć tańsza początkowo, wiązałaby się z koniecznością samodzielnego zarządzania cyklami życia tokenów, obsługą odświeżania tokenów oraz implementacją mechanizmów bezpieczeństwa, co zwiększyłoby ryzyko błędów i podatność na ataki.

\subsubsection{Generowanie danych}

Wybór funkcji \textbf{\mbox{AWS Lambda}} \cite{aws_lambda_docs} jako środowiska wykonawczego dla symulatorów czujników zamiast tradycyjnych podejść kontenerowych czy maszyn wirtualnych był uzasadniony korzyściami ekonomicznymi i skalowalnością modelu bezserwerowego. Tradycyjne podejścia wymagają stałego utrzymania infrastruktury, natomiast funkcja \textbf{\mbox{AWS Lambda}} skaluje się automatycznie w odpowiedzi na zapotrzebowanie, generując koszty tylko za faktyczne zużycie zasobów obliczeniowych. To podejście jest szczególnie efektywne dla symulacji danych, gdzie obciążenie może być nieregularne i nieprzewidywalne.

\subsubsection{Broker wiadomości}

Wybór brokera \textbf{\mbox{Apache Kafka}} \cite{kafka} zamiast alternatywnych brokerów wiadomości, takich jak: broker \textbf{\mbox{RabbitMQ}}, był uzasadniony jego natywną orientacją na przetwarzanie strumieniowe dużych wolumenów danych, co czyni go bardziej odpowiednim dla aplikacji czasu rzeczywistego niż broker \textbf{\mbox{RabbitMQ}}, które zostało zaprojektowane przede wszystkim dla routingu wiadomości w architekturach żądanie-odpowiedź.  Broker \textbf{\mbox{Apache Kafka}} oferuje lepsze gwarancje trwałości danych i skalowalności poziomej, co jest krytyczne w środowiskach przemysłowych charakteryzujących się wysoką zmiennością obciążenia i wymaganiami dotyczącymi niezawodności.

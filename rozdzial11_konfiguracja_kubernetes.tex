\section{Konfiguracja i orkiestracja klastra Kubernetes}
\label{chap:konfiguracja_kubernetes}

Opracowany system, będący przedmiotem niniejszej pracy, jest wdrożony na dwuwęzłowym klastrze Kubernetes, składającym się z węzła głównego (master) oraz węzła roboczego (worker). Został on skonfigurowany w celu zapewnienia wysokiej dostępności, skalowalności oraz bezpieczeństwa. W niniejszym rozdziale przedstawiono kluczowe aspekty tej konfiguracji, koncentrując się na infrastrukturze technicznej, mechanizmach komunikacji oraz wykorzystanych narzędziach i wtyczkach.

Wszystkie aplikacje oraz usługi systemowe zaimplementowane w ramach tego projektu są wdrażane w dedykowanej przestrzeni nazw (namespace) o nazwie \textit{factory}. Zarządzanie manifestami klastra Kubernetes oraz procesem wdrożenia poszczególnych komponentów jest zautomatyzowane przy użyciu narzędzia Helm. Wykorzystano wspólny, reużywalny szablon narzędzia Helm (common chart), zlokalizowany w repozytorium projektu, do standaryzacji definicji zasobów klastra Kubernetes. Ten wspólny chart dostarcza szablonów dla większości typów zasobów, w tym \mbox{Deployment}, \mbox{StatefulSet}, \mbox{Service}, \mbox{ConfigMap}, \mbox{Ingress}, \mbox{PersistentVolume},  \mbox{ServiceAccount}, \\ \mbox{PersistentVolumeClaim},  \mbox{Role}, \mbox{RoleBinding}, \mbox{ClusterRole},  \mbox{ClusterRoleBinding}, \mbox{Job} oraz \\ \mbox{HorizontalPodAutoscaler}, co znacząco upraszcza zarządzanie i zapewnia spójność konfiguracji w całym systemie.

\subsection{Architektura sieciowa}

W zaimplementowanym systemie przekierowywanie ruchu zewnętrznego do usług działających w klastrze jest realizowane za pomocą kontrolera \mbox{Nginx Ingress} (kontroler ruchu przychodzącego). Został on skonfigurowany do obsługi zarówno ruchu \mbox{HTTP/HTTPS}, jak i do przekazywania ruchu TCP (ang. \mbox{Transmission Control Protocol}) \cite{tanenbaum2011computer} dla specyficznych usług, takich jak: broker \mbox{Apache Kafka} oraz baza danych \mbox{PostgreSQL}. Wszystkie usługi zewnętrzne są udostępniane przez ten sam kontroler \mbox{Nginx Ingress}, działający jako dystrybutor obciążenia (ang. \mbox{LoadBalancer}), a jego zewnętrzny adres IP jest punktem wejścia do klastra.

Definicje zasobów Ingress dla poszczególnych usług (np. \textit{front-service.local}, \textit{cluster-ui.local}, \textit{keycloak.local}, \textit{elasticsearch.local}, \textit{schema-registry.local}) wykorzystują lokalne nazwy hostów.

Komunikacja wewnętrzna między serwisami w klastrze odbywa się głównie za pomocą standardowych usług klastra Kubernetes typu ClusterIP. Dla usług stanowych, takich jak: broker \mbox{Apache Kafka} i Zookeeper, wykorzystywane są usługi typu ClusterIP z ustawieniem clusterIP: None (usługi headless), co pozwala na bezpośrednią komunikację z poszczególnymi podami.

\subsection{Zarządzanie konfiguracją aplikacji} 

Konfiguracja poszczególnych aplikacji jest zarządzana za pomocą zasobów ConfigMap. Każda usługa (np. \textit{front-service}, \textit{data-service}, \textit{kafka-db-forwarder}) posiada dedykowany zasób \mbox{ConfigMap}, zawierający plik \textit{application.yaml} ze specyficznymi ustawieniami frameworka \mbox{Spring Boot}, takimi jak: adresy URL usług zależnych, konfiguracja połączeń z bazą danych, brokerem \mbox{Apache Kafka}, czy serwerem autoryzacyjnym Keycloak.
\newpage
Przykładem obiektu mapy konfiguracyjnej (ang. \mbox{ConfigMap}) \textit{front-service-config}, definiującego trasy dla bramy API (\mbox{Spring Cloud Gateway}), jest wskazanie na wewnętrzne usługi takie jak: \textit{data-service-svc} czy \textit{users-service-svc}. Podobnie, \textit{spark-data-processor-config} zawiera szczegółową konfigurację strumieni biblioteki \mbox{Apache Spark Streaming}, w tym definicje okien czasowych, tematów wejściowych i wyjściowych oraz progów dla generowania zdarzeń.

\subsection{Bezpieczeństwo}

Bezpieczeństwo klastra jest wzmocnione przez przedstawione poniżej mechanizmy.

\subsubsection{Zarządzanie certyfikatami TLS}
Do automatycznego zarządzania certyfikatami TLS (ang. \mbox{Transport Layer Security}) w klastrze wykorzystywany jest \textit{cert-manager}, instalowany przy pomocy dostarczonych plików \textit{values.yaml}. Zdefiniowane zasoby Ingress dla usług takich jak: \textit{front-service}, \textit{cluster-ui} oraz serwera Keycloak zawierają sekcje \textit{tls}, które wskazują na sekrety klastra Kubernetes (np. \textit{front-service-tls}, \textit{cluster-ui-tls}) przechowujące certyfikaty i klucze prywatne, zarządzane przez \textit{cert-manager}.

\subsubsection{Kontrola dostępu oparta na rolach (RBAC)}
\label{subsubsec:RBAC}
Opracowany system wykorzystuje mechanizmy kontroli dostępu oparte na rolach (RBAC, ang. \mbox{Role-Based Access Control}) do definiowania uprawnień dla poszczególnych komponentów. Przykładem jest \textit{front-service}, dla którego zdefiniowano zasobach ServiceAccount (\textit{front-service-sa}), Role (\textit{front-service-role}) oraz RoleBinding (\textit{front-service-rolebinding}). Rola ta nadaje uprawnienia do odczytu i listowania podów oraz deploymentów, a także do tworzenia podów i ich wykonywania (pods/exec), co jest wykorzystywane w zadaniu (Job) \textit{add-keycloak-host}.

Operator Spark również posiada własną konfigurację RBAC, definiującą uprawnienia niezbędne do zarządzania aplikacjami silnika analitycznego Spark w przestrzeniach nazw \textit{factory} oraz \textit{default}.

\subsubsection{Serwer autoryzacyjny Keycloak}
Serwer Keycloak jest wdrożony jako centralny serwer uwierzytelniania i autoryzacji. Jego konfiguracja obejmuje połączenie z bazą danych PostgreSQL (\textit{database-svc}) oraz dane logowania administratora. Dostęp do interfejsu API serwera Keycloak jest zapewniony poprzez kontroler Ingress na adresie \textit{keycloak.local}.

\subsection{Platforma danych}

Kluczowe komponenty platformy danych to broker brokera Kafki, bazy danych Elasticsearch oraz PostgreSQL. Wszystkie te usługi wykorzystują mechanizmy przechowywania danych bezpośrednio na węzłach klastra.

\subsubsection{Broker Apache Kafka i serwer Schema Registry}
Klaster Apache Kafka składa się z trzech brokerów oraz instancji Zookeeper. Każdy z nich jest wdrażany jako zasób StatefulSet i wykorzystuje wolumeny typu hostPath do przechowywania danych bezpośrednio na fizycznych węzłach klastra: Zookeeper na węźle master, a brokery Apache Kafka częściowo na węźle roboczym i masterze. Taka konfiguracja przypina konkretne instancje do fizycznych maszyn. Komunikacja z brokerami odbywa się poprzez dedykowane usługi \mbox{ClusterIP} (usługi headless dla komunikacji wewnętrznej i standardowe dla ruchu zewnętrznego poprzez kontroler Nginx Ingress z mapowaniem portów TCP).

Usługa Confluent Schema Registry jest również wdrożona i połączona z serwerem Zookeeper oraz brokerami Kafki, umożliwiając zarządzanie schematami Avro. Dostęp do usługi Schema Registry jest zapewniony przez kontroler Ingress na adresie \textit{schema-registry.local}. Po uruchomieniu klastra Kafki, uruchamiane jest zadanie (Job) \textit{create-topics}, które automatycznie tworzy predefiniowaną listę tematów z odpowiednią liczbą partycji i współczynnikiem replikacji.

\subsubsection{Elasticsearch}
Baza danych Elasticsearch jest wdrażany jako zasób StatefulSet z jedną repliką, skonfigurowana do pracy w trybie \textit{single-node}. Dane bazy danych Elasticsearch są przechowywane na wolumenie trwałym (PersistentVolume) typu \textit{local-storage}, zmapowanym do konkretnej ścieżki na węźle roboczym. Dostęp do bazy danych Elasticsearch jest możliwy poprzez usługę ClusterIP oraz Ingress na adresie \textit{elasticsearch.local}.

\subsubsection{PostgreSQL}
Baza danych PostgreSQL (wersja 16) jest wdrażana jako zasób StatefulSet. Podobnie jak baza danych Elasticsearch, wykorzystuje wolumen trwały typu \textit{local-storage} zmapowany do ścieżki na węźle roboczym do przechowywania danych. Konfiguracja bazy, w tym dane logowania, jest dostarczana poprzez zasób \mbox{ConfigMap} o nazwie \textit{db-credentials}. Baza danych jest wykorzystywana przez serwer Keycloak oraz inne usługi aplikacyjne (np. \textit{data-service}, \textit{users-service}, \textit{kafka-db-forwarder}). Dostęp do bazy danych wewnątrz klastra zapewnia usługa \textit{database-svc}, a ruch zewnętrzny jest możliwy dzięki mapowaniu portu TCP w konfiguracji kontrolera \mbox{Nginx Ingress}.

\subsection{Przetwarzanie danych i logika aplikacyjna}

\subsubsection{Spark Operator}
Do zarządzania i uruchamiania zadań silnika analitycznego \mbox{Apache Spark} w klastrze wykorzystywany jest zasób \mbox{Spark Operator}. Jego konfiguracja obejmuje definicję kontrolera, webhooka oraz odpowiednich ról RBAC. Operator monitoruje zasoby \mbox{SparkApplication} i zarządza cyklem życia sterowników (ang. driver) i wykonawców (ang. executor) silnika Spark.

Zdefiniowany jest zasób SparkApplication o nazwie \textit{spark-data-processor}, uruchamiający aplikację Scala (\textit{com.factory.SparkDataProcessor}) z obrazu Dockera. Aplikacja ta działa w trybie \textit{cluster} i wykorzystuje zasoby zdefiniowane dla sterownika i dwóch wykonawców (executorów).

\subsubsection{Usługi aplikacyjne}
Pozostałe usługi, takie jak: \textit{front-service} (brama API), \textit{cluster-ui} (interfejs użytkownika React), \textit{data-service}, \textit{users-service}, \textit{kafka-db-forwarder} oraz \textit{sqs-kafka-forwarder} są wdrażane jako standardowe Deploymenty klastra Kubernetes. Każda z nich posiada własną konfigurację (ConfigMap), usługę (Service) oraz, w przypadku usług front-endowych i bramy API, definicję Ingress.

Usługa \textit{sqs-kafka-forwarder} jest skonfigurowana do integracji z rzeczywistymi kolejkami Amazon SQS i przekazywania wiadomości do odpowiednich tematów brokera Kafki. Konfiguracja zawiera klucze dostępowe chmury AWS, co potwierdza bezpośrednią integrację z infrastrukturą chmury AWS.

Dodatkowym elementem jest zadanie (Job) \textit{add-keycloak-host} powiązane z \textit{front-service}. Zadanie to, po udanym wdrożeniu \textit{front-service}, modyfikuje plik \textit{/etc/hosts} w podach \\ \textit{front-service}, dodając wpis mapujący \textit{keycloak.local} na adres IP kontrolera Ingress. Jest to obejście problemów z rozwiązywaniem nazw DNS dla serwera Keycloak z poziomu tej usługi, zapewniając bezpośrednią komunikację po adresie IP kontrolera Ingress.

\subsection{Zarządzanie zasobami i sondy}
\label{sub:zasoby_sondy}
Wiele wdrożeń (Deployments) i StatefulSetów definiuje żądania (requests) i limity (limits) zasobów CPU i pamięci, co jest dobrą praktyką zapewniającą stabilność klastra. Przykładowo, \textit{front-service} ma zdefiniowane limity na 500m CPU i 512Mi pamięci, a serwer Keycloak na 1000m CPU i 1Gi pamięci.

Dodatkowo, dla kluczowych usług takich jak: \textit{front-service}, \textit{cluster-ui} oraz serwer Keycloak, zdefiniowane są sondy żywotności (livenessProbe) i gotowości (readinessProbe). Sondy te pozwalają klastrowi Kubernetes na monitorowanie stanu aplikacji i automatyczne restartowanie kontenerów lub kierowanie ruchu tylko do zdrowych instancji.

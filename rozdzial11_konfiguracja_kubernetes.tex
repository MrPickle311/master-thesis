\section{Konfiguracja i orkiestracja klastra Kubernetes}
\label{chap:konfiguracja_kubernetes}
\addcontentsline{lof}{section}{Rozdział \ref{chap:konfiguracja_kubernetes}}

System będący przedmiotem niniejszej pracy jest wdrożony na dwuwęzłowym klastrze klastra Kubernetes, składającym się z węzła głównego (master) oraz węzła roboczego (worker). Klaster został skonfigurowany w celu zapewnienia wysokiej dostępności, skalowalności oraz bezpieczeństwa. W niniejszym rozdziale przedstawiono kluczowe aspekty tej konfiguracji, koncentrując się na infrastrukturze technicznej, mechanizmach komunikacji oraz wykorzystanych narzędziach i wtyczkach.

Wszystkie aplikacje oraz usługi systemowe zaimplementowane w ramach tego projektu są wdrażane w dedykowanej przestrzeni nazw (namespace) o nazwie \texttt{factory}. Zarządzanie manifestami klastra Kubernetes oraz procesem wdrożenia poszczególnych komponentów jest zautomatyzowane przy użyciu narzędzia Helm. Wykorzystano wspólny, reużywalny szablon narzędzia Helm (common chart), zlokalizowany w repozytorium projektu, do standaryzacji definicji zasobów klastra Kubernetes. Ten wspólny chart dostarcza szablonów dla większości typów zasobów, w tym \texttt{Deployment}, \texttt{StatefulSet}, \texttt{Service}, \texttt{ConfigMap}, \texttt{Ingress}, \texttt{PersistentVolume}, \texttt{PersistentVolumeClaim}, \texttt{ServiceAccount}, \texttt{Role}, \texttt{RoleBinding}, \texttt{ClusterRole},  \texttt{ClusterRoleBinding}, \texttt{Job} oraz \texttt{HorizontalPodAutoscaler}, co znacząco upraszcza zarządzanie i zapewnia spójność konfiguracji w całym systemie.

\subsection{Architektura sieciowa}

W zaimplementowanym systemie przekierowywanie ruchu zewnętrznego do usług działających w klastrze jest realizowane za pomocą kontrolera Nginx Ingress. Kontroler ten jest skonfigurowany do obsługi zarówno ruchu HTTP/HTTPS, jak i do przekazywania ruchu TCP (ang. Transmission Control Protocol) dla specyficznych usług, takich jak: broker Kafka oraz baza danych PostgreSQL. Wszystkie usługi zewnętrzne są udostępniane przez ten sam kontroler Nginx Ingress, działający jako dystrybutor obciążenia (ang. LoadBalancer), a jego zewnętrzny adres IP jest punktem wejścia do klastra.

Definicje Ingress dla poszczególnych usług (np. \texttt{front-service.local}, \texttt{cluster-ui.local}, \texttt{keycloak.local}, \texttt{elasticsearch.local}, \texttt{schema-registry.local}) wykorzystują lokalne nazwy hostów.

Komunikacja wewnętrzna między serwisami w klastrze odbywa się głównie za pomocą standardowych usług klastra Kubernetes typu \texttt{ClusterIP}. Dla usług stanowych, takich jak: broker Kafka i Zookeeper, wykorzystywane są usługi typu \texttt{ClusterIP} z ustawieniem \texttt{clusterIP: None} (usługi headless), co pozwala na bezpośrednią komunikację z poszczególnymi podami.

\subsection{Zarządzanie konfiguracją aplikacji} 

Konfiguracja poszczególnych aplikacji jest zarządzana za pomocą zasobów \texttt{ConfigMap}. Każda usługa (np. \texttt{front-service}, \texttt{data-service}, \texttt{kafka-db-forwarder}) posiada dedykowany \texttt{ConfigMap}, który zawiera plik \texttt{application.yaml} ze specyficznymi ustawieniami frameworka Spring Boot, takimi jak adresy URL usług zależnych, konfiguracja połączeń z bazą danych, brokerem brokera Kafki, czy serwerem autoryzacyjnym Keycloak.

Przykładem obiektu mapy konfiguracyjnej (ang. \textit{ConfigMap}) \texttt{front-service-config}, który definiuje trasy dla bramy API (Spring Cloud Gateway), wskazując na wewnętrzne usługi takie jak: \texttt{data-service-svc} czy \texttt{users-service-svc}. Podobnie, \texttt{kafka-data-processor-config} zawiera szczegółową konfigurację strumieni biblioteki Kafka Streams, w tym definicje okien czasowych, tematów wejściowych i wyjściowych oraz progów dla generowania zdarzeń.

\subsection{Bezpieczeństwo}

Bezpieczeństwo klastra jest wzmocnione przez przedstawione poniżej mechanizmy:

\subsubsection{Zarządzanie certyfikatami TLS}
Do automatycznego zarządzania certyfikatami TLS (ang. Transport Layer Security) w klastrze wykorzystywany jest \texttt{cert-manager}, instalowany przy pomocy dostarczonych plików \texttt{values.yaml}. Zdefiniowane zasoby Ingress dla usług takich jak: \texttt{front-service}, \texttt{cluster-ui} oraz \texttt{keycloak} zawierają sekcje \texttt{tls}, które wskazują na sekrety klastra Kubernetes (np. \texttt{front-service-tls}, \texttt{cluster-ui-tls}) przechowujące certyfikaty i klucze prywatne, zarządzane przez \texttt{cert-manager}.

\subsubsection{Kontrola dostępu oparta na rolach (RBAC)}
System wykorzystuje mechanizmy kontroli dostępu oparte na rolach (RBAC, ang. Role-Based Access Control) do definiowania uprawnień dla poszczególnych komponentów. Przykładem jest \texttt{front-service}, dla którego zdefiniowano \texttt{ServiceAccount} (\texttt{front-service-sa}), \texttt{Role} (\texttt{front-service-role}) oraz \texttt{RoleBinding} \\ (\texttt{front-service-rolebinding}). Rola ta nadaje uprawnienia do odczytu i listowania podów oraz deploymentów, a także do tworzenia podów i ich wykonywania (pods/exec), co jest wykorzystywane w zadaniu (Job) \texttt{add-keycloak-host}.

Operator Spark również posiada własną konfigurację RBAC, definiującą uprawnienia niezbędne do zarządzania aplikacjami silnika analitycznego Spark w przestrzeniach nazw \texttt{factory} oraz \texttt{default}.

\subsubsection{Serwer autoryzacyjny Keycloak}
Serwer Keycloak jest wdrożony jako centralny serwer uwierzytelniania i autoryzacji. Jego konfiguracja obejmuje połączenie z bazą danych PostgreSQL (\texttt{database-svc}) oraz dane logowania administratora. Dostęp do interfejsu API serwera Keycloak jest zapewniony poprzez kontroler Ingress na adresie \texttt{keycloak.local}.

\subsection{Platforma danych}

Kluczowe komponenty platformy danych to broker brokera Kafki, bazy danych Elasticsearch oraz PostgreSQL. Wszystkie te usługi wykorzystują mechanizmy przechowywania danych bezpośrednio na węzłach klastra.

\subsubsection{Broker Apache Kafka i serwer Schema Registry}
Klaster Kafki składa się z trzech brokerów oraz instancji Zookeepera. Każdy broker oraz Zookeeper są wdrażane jako \texttt{StatefulSet} i wykorzystują wolumeny typu \texttt{hostPath} do przechowywania danych bezpośrednio na fizycznych węzłach klastra: Zookeeper na węźle master, a brokery platformy Kafka częściowo na węźle roboczym i masterze. Taka konfiguracja przypina konkretne instancje do fizycznych maszyn. Komunikacja z brokerami odbywa się poprzez dedykowane usługi \texttt{ClusterIP} (usługi headless dla komunikacji wewnętrznej i standardowe dla ruchu zewnętrznego poprzez kontroler Nginx Ingress z mapowaniem portów TCP).

Usługa Confluent Schema Registry jest również wdrożona i połączona z serwerem Zookeeper oraz brokerami Kafki, umożliwiając zarządzanie schematami Avro. Dostęp do usługi Schema Registry jest zapewniony przez kontroler Ingress na adresie \texttt{schema-registry.local}. Po uruchomieniu klastra Kafki, uruchamiane jest zadanie (Job) \texttt{create-topics}, które automatycznie tworzy predefiniowaną listę tematów z odpowiednią liczbą partycji i współczynnikiem replikacji.

\subsubsection{Elasticsearch}
Baza danych Elasticsearch jest wdrażany jako \texttt{StatefulSet} z jedną repliką, skonfigurowana do pracy w trybie \texttt{single-node}. Dane bazy danych Elasticsearch są przechowywane na wolumenie trwałym (PersistentVolume) typu \texttt{local-storage}, zmapowanym do konkretnej ścieżki na węźle roboczym. Dostęp do bazy danych Elasticsearch jest możliwy poprzez usługę \texttt{ClusterIP} oraz Ingress na adresie \texttt{elasticsearch.local}.

\subsubsection{PostgreSQL}
Baza danych PostgreSQL (wersja 16) jest wdrażana jako \texttt{StatefulSet}. Podobnie jak baza danych Elasticsearch, wykorzystuje wolumen trwały typu \texttt{local-storage} zmapowany do ścieżki na węźle roboczym do przechowywania danych. Konfiguracja bazy, w tym dane logowania, jest dostarczana poprzez \texttt{ConfigMap} o nazwie \texttt{db-credentials}. Baza danych jest wykorzystywana przez system Keycloak oraz inne usługi aplikacyjne (np. \texttt{data-service}, \texttt{users-service}, \texttt{kafka-db-forwarder}). Dostęp do bazy danych wewnątrz klastra zapewnia usługa \texttt{database-svc}, a ruch zewnętrzny jest możliwy dzięki mapowaniu portu TCP w konfiguracji kontrolera Nginx Ingress.

\subsection{Przetwarzanie danych i logika aplikacyjna}

\subsubsection{Spark Operator}
Do zarządzania i uruchamiania zadań silnika analitycznego Apache Spark w klastrze wykorzystywany jest Spark Operator. Jego konfiguracja obejmuje definicję kontrolera, webhooka oraz odpowiednich ról RBAC. Operator monitoruje zasoby \texttt{SparkApplication} i zarządza cyklem życia sterowników (driver) i wykonawców (executor) silnika Spark.

Zdefiniowany jest zasób \texttt{SparkApplication} o nazwie \texttt{spark-data-processor}, który uruchamia aplikację Scala (\texttt{com.factory.SparkDataProcessor}) z obrazu Dockera. Aplikacja ta działa w trybie \texttt{cluster} i wykorzystuje zasoby zdefiniowane dla sterownika i dwóch wykonawców (executorów).

\subsubsection{Usługi aplikacyjne}
Pozostałe usługi, takie jak: \texttt{front-service} (brama API), \texttt{cluster-ui} (interfejs użytkownika React), \texttt{data-service}, \texttt{users-service}, \texttt{kafka-db-forwarder} oraz \texttt{sqs-kafka-forwarder} są wdrażane jako standardowe Deploymenty klastra Kubernetes. Każda z nich posiada własną konfigurację (ConfigMap), usługę (Service) oraz, w przypadku usług frontendowych i bramy API, definicję Ingress.

Usługa \texttt{sqs-kafka-forwarder} jest skonfigurowana do integracji z rzeczywistymi kolejkami Amazon SQS i przekazywania wiadomości do odpowiednich tematów brokera Kafki. Konfiguracja zawiera klucze dostępowe chmury AWS, co potwierdza bezpośrednią integrację z infrastrukturą chmury AWS.

Dodatkowym elementem jest zadanie (Job) \texttt{add-keycloak-host} powiązane z \texttt{front-service}. Zadanie to, po udanym wdrożeniu \texttt{front-service}, modyfikuje plik \texttt{/etc/hosts} w podach \\ \texttt{front-service}, dodając wpis mapujący \texttt{keycloak.local} na adres IP kontrolera Ingress. Jest to obejście problemów z rozwiązywaniem nazw DNS dla serwera Keycloak z poziomu tej usługi, zapewniając bezpośrednią komunikację po adresie IP kontrolera Ingress.

\subsection{Zarządzanie zasobami i sondy}

Wiele wdrożeń (Deployments) i StatefulSetów definiuje żądania (requests) i limity (limits) zasobów CPU i pamięci, co jest dobrą praktyką zapewniającą stabilność klastra. Przykładowo, \texttt{front-service} ma zdefiniowane limity na 500m CPU i 512Mi pamięci, a \texttt{keycloak} na 1000m CPU i 1Gi pamięci.

Dodatkowo, dla kluczowych usług takich jak: \texttt{front-service}, \texttt{cluster-ui} oraz \texttt{keycloak}, zdefiniowane są sondy żywotności (livenessProbe) i gotowości (readinessProbe). Sondy te pozwalają klastrowi Kubernetes na monitorowanie stanu aplikacji i automatyczne restartowanie kontenerów lub kierowanie ruchu tylko do zdrowych instancji.
